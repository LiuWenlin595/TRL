# ç›‘ç£å¾®è°ƒè®­ç»ƒå™¨

[![](https://img.shields.io/badge/All_models-SFT-blue)](https://huggingface.co/models?other=sft,trl) [![](https://img.shields.io/badge/smol_course-Chapter_1-yellow)](https://github.com/huggingface/smol-course/tree/main/1_instruction_tuning)

ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ˜¯åè®­ç»ƒåŸºç¡€æ¨¡å‹ä¸­æœ€å¸¸è§çš„æ­¥éª¤ï¼Œä¹Ÿæ˜¯æœ€æœ‰æ•ˆçš„æ­¥éª¤ä¹‹ä¸€ã€‚åœ¨TRLä¸­ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªç®€å•çš„APIï¼Œå¯ä»¥ç”¨å‡ è¡Œä»£ç è®­ç»ƒSFTæ¨¡å‹ï¼›å¯¹äºå®Œæ•´çš„è®­ç»ƒè„šæœ¬ï¼Œè¯·æŸ¥çœ‹[`trl/scripts/sft.py`](https://github.com/huggingface/trl/tree/main/trl/scripts/sft.py)ã€‚è§†è§‰è¯­è¨€æ¨¡å‹çš„å®éªŒæ€§æ”¯æŒä¹ŸåŒ…å«åœ¨[`examples/scripts/sft_vlm.py`](https://github.com/huggingface/trl/tree/main/examples/scripts/sft_vlm.py)ä¸­ã€‚

## å¿«é€Ÿå¼€å§‹

å¦‚æœæ‚¨æœ‰ä¸€ä¸ªæ‰˜ç®¡åœ¨ğŸ¤— Hubä¸Šçš„æ•°æ®é›†ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨TRLçš„[`SFTTrainer`]è½»æ¾å¾®è°ƒæ‚¨çš„SFTæ¨¡å‹ã€‚å‡è®¾æ‚¨çš„æ•°æ®é›†æ˜¯`imdb`ï¼Œæ‚¨æƒ³è¦é¢„æµ‹çš„æ–‡æœ¬åœ¨æ•°æ®é›†çš„`text`å­—æ®µä¸­ï¼Œæ‚¨æƒ³è¦å¾®è°ƒ`facebook/opt-350m`æ¨¡å‹ã€‚
ä»¥ä¸‹ä»£ç ç‰‡æ®µä¸ºæ‚¨å¤„ç†æ‰€æœ‰çš„æ•°æ®é¢„å¤„ç†å’Œè®­ç»ƒï¼š

```python
from datasets import load_dataset
from trl import SFTConfig, SFTTrainer

dataset = load_dataset("stanfordnlp/imdb", split="train")

training_args = SFTConfig(
    max_length=512,
    output_dir="/tmp",
)
trainer = SFTTrainer(
    "facebook/opt-350m",
    train_dataset=dataset,
    args=training_args,
)
trainer.train()
```
ç¡®ä¿ä¸º`max_length`ä¼ é€’æ­£ç¡®çš„å€¼ï¼Œå› ä¸ºé»˜è®¤å€¼å°†è®¾ç½®ä¸º`min(tokenizer.model_max_length, 1024)`ã€‚

æ‚¨ä¹Ÿå¯ä»¥åœ¨è®­ç»ƒå™¨å¤–éƒ¨æ„å»ºæ¨¡å‹å¹¶æŒ‰å¦‚ä¸‹æ–¹å¼ä¼ é€’ï¼š

```python
from transformers import AutoModelForCausalLM
from datasets import load_dataset
from trl import SFTConfig, SFTTrainer

dataset = load_dataset("stanfordnlp/imdb", split="train")

model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m")

training_args = SFTConfig(output_dir="/tmp")

trainer = SFTTrainer(
    model,
    train_dataset=dataset,
    args=training_args,
)

trainer.train()
```

ä¸Šè¿°ä»£ç ç‰‡æ®µå°†ä½¿ç”¨[`SFTConfig`]ç±»ä¸­çš„é»˜è®¤è®­ç»ƒå‚æ•°ã€‚å¦‚æœæ‚¨æƒ³ä¿®æ”¹é»˜è®¤å€¼ï¼Œè¯·å°†æ‚¨çš„ä¿®æ”¹ä¼ é€’ç»™`SFTConfig`æ„é€ å‡½æ•°ï¼Œå¹¶é€šè¿‡`args`å‚æ•°å°†å…¶ä¼ é€’ç»™è®­ç»ƒå™¨ã€‚

## é«˜çº§ç”¨æ³•

### ä»…åœ¨å®Œæˆç»“æœä¸Šè®­ç»ƒ

è¦ä»…åœ¨å®Œæˆç»“æœä¸Šè®­ç»ƒï¼Œåªéœ€ä½¿ç”¨[æç¤º-å®Œæˆ](dataset_formats#prompt-completion)æ•°æ®é›†ã€‚åœ¨æ­¤æ¨¡å¼ä¸‹ï¼ŒæŸå¤±ä»…è®¡ç®—åœ¨å®Œæˆéƒ¨åˆ†ã€‚

å¦‚æœæ‚¨æƒ³åœ¨æç¤º**å’Œ**å®Œæˆç»“æœä¸Šéƒ½è®¡ç®—æŸå¤±ï¼ŒåŒæ—¶ä»ä½¿ç”¨æç¤º-å®Œæˆæ•°æ®é›†ï¼Œè¯·åœ¨[`SFTConfig`]ä¸­è®¾ç½®`completion_only_loss=False`ã€‚è¿™ç›¸å½“äº[å°†æ•°æ®é›†è½¬æ¢ä¸ºè¯­è¨€å»ºæ¨¡](dataset_formats#from-prompt-completion-to-language-modeling-dataset)æ ¼å¼ã€‚

### ä¸ºèŠå¤©æ ¼å¼æ·»åŠ ç‰¹æ®Šæ ‡è®°

å‘è¯­è¨€æ¨¡å‹æ·»åŠ ç‰¹æ®Šæ ‡è®°å¯¹äºè®­ç»ƒèŠå¤©æ¨¡å‹è‡³å…³é‡è¦ã€‚è¿™äº›æ ‡è®°æ·»åŠ åœ¨å¯¹è¯ä¸­ä¸åŒè§’è‰²ä¹‹é—´ï¼Œå¦‚ç”¨æˆ·ã€åŠ©æ‰‹å’Œç³»ç»Ÿï¼Œå¸®åŠ©æ¨¡å‹è¯†åˆ«å¯¹è¯çš„ç»“æ„å’Œæµç¨‹ã€‚è¿™ç§è®¾ç½®å¯¹äºä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨èŠå¤©ç¯å¢ƒä¸­ç”Ÿæˆè¿è´¯ä¸”ä¸Šä¸‹æ–‡é€‚å½“çš„å“åº”è‡³å…³é‡è¦ã€‚
[`clone_chat_template`]å‡½æ•°æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„å·¥å…·ï¼Œç”¨äºä¸ºå¯¹è¯AIä»»åŠ¡å‡†å¤‡æ¨¡å‹å’Œåˆ†è¯å™¨ã€‚æ­¤å‡½æ•°ï¼š
- å‘åˆ†è¯å™¨æ·»åŠ ç‰¹æ®Šæ ‡è®°ï¼Œä¾‹å¦‚`<|im_start|>`å’Œ`<|im_end|>`ï¼Œä»¥æŒ‡ç¤ºå¯¹è¯çš„å¼€å§‹å’Œç»“æŸã€‚
- è°ƒæ•´æ¨¡å‹çš„åµŒå…¥å±‚ä»¥é€‚åº”æ–°æ ‡è®°ã€‚
- è®¾ç½®åˆ†è¯å™¨çš„`chat_template`ï¼Œç”¨äºå°†è¾“å…¥æ•°æ®æ ¼å¼åŒ–ä¸ºç±»ä¼¼èŠå¤©çš„æ ¼å¼ã€‚
- _å¯é€‰åœ°_ï¼Œæ‚¨å¯ä»¥ä¼ é€’`resize_to_multiple_of`æ¥å°†åµŒå…¥å±‚è°ƒæ•´ä¸º`resize_to_multiple_of`å‚æ•°çš„å€æ•°ï¼Œä¾‹å¦‚`64`ã€‚å¦‚æœæ‚¨å¸Œæœ›åœ¨æœªæ¥çœ‹åˆ°æ›´å¤šæ ¼å¼çš„æ”¯æŒï¼Œè¯·åœ¨[trl](https://github.com/huggingface/trl)ä¸Šæ‰“å¼€GitHubé—®é¢˜

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from trl import clone_chat_template

# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m")
tokenizer = AutoTokenizer.from_pretrained("facebook/opt-350m")

# è®¾ç½®èŠå¤©æ ¼å¼
model, tokenizer = clone_chat_template(model, tokenizer, "Qwen/Qwen3-0.6B")
```

> [!WARNING]
> ä¸€äº›åŸºç¡€æ¨¡å‹ï¼Œå¦‚æ¥è‡ªQwençš„æ¨¡å‹ï¼Œåœ¨æ¨¡å‹çš„åˆ†è¯å™¨ä¸­æœ‰é¢„å®šä¹‰çš„èŠå¤©æ¨¡æ¿ã€‚åœ¨è¿™äº›æƒ…å†µä¸‹ï¼Œä¸éœ€è¦åº”ç”¨[`clone_chat_template()`]ï¼Œå› ä¸ºåˆ†è¯å™¨å·²ç»å¤„ç†äº†æ ¼å¼åŒ–ã€‚ä½†æ˜¯ï¼Œéœ€è¦å°†EOSæ ‡è®°ä¸èŠå¤©æ¨¡æ¿å¯¹é½ï¼Œä»¥ç¡®ä¿æ¨¡å‹çš„å“åº”æ­£ç¡®ç»ˆæ­¢ã€‚åœ¨è¿™äº›æƒ…å†µä¸‹ï¼Œåœ¨[`SFTConfig`]ä¸­æŒ‡å®š`eos_token`ï¼›ä¾‹å¦‚ï¼Œå¯¹äº`Qwen/Qwen2.5-1.5B`ï¼Œåº”è¯¥è®¾ç½®`eos_token="<|im_end|>"`ã€‚

è®¾ç½®å¥½æ¨¡å‹å’Œåˆ†è¯å™¨åï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥åœ¨å¯¹è¯æ•°æ®é›†ä¸Šå¾®è°ƒæˆ‘ä»¬çš„æ¨¡å‹ã€‚ä»¥ä¸‹æ˜¯æ•°æ®é›†å¦‚ä½•æ ¼å¼åŒ–ä¸ºå¾®è°ƒçš„ç¤ºä¾‹ã€‚

### æ•°æ®é›†æ ¼å¼æ”¯æŒ

[`SFTTrainer`]æ”¯æŒæµè¡Œçš„æ•°æ®é›†æ ¼å¼ã€‚è¿™å…è®¸æ‚¨ç›´æ¥å°†æ•°æ®é›†ä¼ é€’ç»™è®­ç»ƒå™¨ï¼Œæ— éœ€ä»»ä½•é¢„å¤„ç†ã€‚æ”¯æŒä»¥ä¸‹æ ¼å¼ï¼š
* å¯¹è¯æ ¼å¼
```json
{"messages": [{"role": "system", "content": "You are helpful"}, {"role": "user", "content": "What's the capital of France?"}, {"role": "assistant", "content": "..."}]}
{"messages": [{"role": "system", "content": "You are helpful"}, {"role": "user", "content": "Who wrote 'Romeo and Juliet'?"}, {"role": "assistant", "content": "..."}]}
{"messages": [{"role": "system", "content": "You are helpful"}, {"role": "user", "content": "How far is the Moon from Earth?"}, {"role": "assistant", "content": "..."}]}
```
* æŒ‡ä»¤æ ¼å¼
```json
{"prompt": "<prompt text>", "completion": "<ideal generated text>"}
{"prompt": "<prompt text>", "completion": "<ideal generated text>"}
{"prompt": "<prompt text>", "completion": "<ideal generated text>"}
```

å¦‚æœæ‚¨çš„æ•°æ®é›†ä½¿ç”¨ä¸Šè¿°æ ¼å¼ä¹‹ä¸€ï¼Œæ‚¨å¯ä»¥ç›´æ¥å°†å…¶ä¼ é€’ç»™è®­ç»ƒå™¨ï¼Œæ— éœ€é¢„å¤„ç†ã€‚[`SFTTrainer`]å°†ä½¿ç”¨æ¨¡å‹åˆ†è¯å™¨ä¸­å®šä¹‰çš„æ ¼å¼ï¼Œé€šè¿‡[apply_chat_template](https://huggingface.co/docs/transformers/main/en/chat_templating#templates-for-chat-models)æ–¹æ³•ä¸ºæ‚¨æ ¼å¼åŒ–æ•°æ®é›†ã€‚

```python
from datasets import load_dataset
from trl import SFTConfig, SFTTrainer

...

# åŠ è½½jsonlæ•°æ®é›†
dataset = load_dataset("json", data_files="path/to/dataset.jsonl", split="train")
# ä»HuggingFace HubåŠ è½½æ•°æ®é›†
dataset = load_dataset("philschmid/dolly-15k-oai-style", split="train")

...

training_args = SFTConfig(packing=True)
trainer = SFTTrainer(
    "facebook/opt-350m",
    args=training_args,
    train_dataset=dataset,
)
```

å¦‚æœæ•°æ®é›†ä¸æ˜¯è¿™äº›æ ¼å¼ä¹‹ä¸€ï¼Œæ‚¨å¯ä»¥é¢„å¤„ç†æ•°æ®é›†ä»¥åŒ¹é…æ ¼å¼åŒ–ï¼Œæˆ–è€…å‘SFTTrainerä¼ é€’æ ¼å¼åŒ–å‡½æ•°æ¥ä¸ºæ‚¨å®Œæˆã€‚è®©æˆ‘ä»¬çœ‹çœ‹ã€‚

### æ ¼å¼åŒ–æ‚¨çš„è¾“å…¥æç¤º

å¯¹äºæŒ‡ä»¤å¾®è°ƒï¼Œåœ¨æ•°æ®é›†ä¸­æœ‰ä¸¤ä¸ªåˆ—æ˜¯å¾ˆå¸¸è§çš„ï¼šä¸€ä¸ªç”¨äºæç¤ºï¼Œå¦ä¸€ä¸ªç”¨äºå“åº”ã€‚
è¿™å…è®¸äººä»¬åƒ[Stanford-Alpaca](https://github.com/tatsu-lab/stanford_alpaca)é‚£æ ·æ ¼å¼åŒ–ç¤ºä¾‹ï¼š
```bash
Below is an instruction ...

### Instruction
{prompt}

### Response:
{completion}
```
å‡è®¾æ‚¨çš„æ•°æ®é›†æœ‰ä¸¤ä¸ªå­—æ®µï¼Œ`question`å’Œ`answer`ã€‚å› æ­¤æ‚¨å¯ä»¥è¿è¡Œï¼š
```python
...
def formatting_prompts_func(example):
    return f"### Question: {example['question']}\n ### Answer: {example['answer']}"


trainer = SFTTrainer(
    model,
    args=training_args,
    train_dataset=dataset,
    formatting_func=formatting_prompt_func,
)

trainer.train()
```
è¦æ­£ç¡®æ ¼å¼åŒ–æ‚¨çš„è¾“å…¥ï¼Œè¯·ç¡®ä¿é€šè¿‡å¾ªç¯éå†æ‰€æœ‰ç¤ºä¾‹å¹¶è¿”å›å¤„ç†åçš„æ–‡æœ¬åˆ—è¡¨æ¥å¤„ç†æ‰€æœ‰ç¤ºä¾‹ã€‚æŸ¥çœ‹å¦‚ä½•åœ¨alpacaæ•°æ®é›†ä¸Šä½¿ç”¨SFTTrainerçš„å®Œæ•´ç¤ºä¾‹[è¿™é‡Œ](https://github.com/huggingface/trl/pull/444#issue-1760952763)

### æ‰“åŒ…æ•°æ®é›†

[`SFTTrainer`]æ”¯æŒ_ç¤ºä¾‹æ‰“åŒ…_ï¼Œå…¶ä¸­å¤šä¸ªçŸ­ç¤ºä¾‹æ‰“åŒ…åœ¨åŒä¸€ä¸ªè¾“å…¥åºåˆ—ä¸­ä»¥æé«˜è®­ç»ƒæ•ˆç‡ã€‚è¦å¯ç”¨æ­¤æ•°æ®é›†ç±»çš„ä½¿ç”¨ï¼Œåªéœ€å°†`packing=True`ä¼ é€’ç»™[`SFTConfig`]æ„é€ å‡½æ•°ã€‚

```python
...
training_args = SFTConfig(packing=True)

trainer = SFTTrainer(
    "facebook/opt-350m",
    train_dataset=dataset,
    args=training_args
)

trainer.train()
```

è¯·æ³¨æ„ï¼Œå¦‚æœæ‚¨ä½¿ç”¨æ‰“åŒ…æ•°æ®é›†å¹¶ä¸”å¦‚æœæ‚¨åœ¨è®­ç»ƒå‚æ•°ä¸­ä¼ é€’`max_steps`ï¼Œæ‚¨å¯èƒ½ä¼šè®­ç»ƒæ¨¡å‹è¶…è¿‡å‡ ä¸ªepochï¼Œå…·ä½“å–å†³äºæ‚¨é…ç½®æ‰“åŒ…æ•°æ®é›†å’Œè®­ç»ƒåè®®çš„æ–¹å¼ã€‚è¯·ä»”ç»†æ£€æŸ¥æ‚¨çŸ¥é“å¹¶ç†è§£æ‚¨åœ¨åšä»€ä¹ˆã€‚
å¦‚æœæ‚¨ä¸æƒ³æ‰“åŒ…æ‚¨çš„`eval_dataset`ï¼Œæ‚¨å¯ä»¥åœ¨`SFTConfig`åˆå§‹åŒ–æ–¹æ³•ä¸­ä¼ é€’`eval_packing=False`ã€‚

#### ä½¿ç”¨æ‰“åŒ…æ•°æ®é›†è‡ªå®šä¹‰æ‚¨çš„æç¤º

å¦‚æœæ‚¨çš„æ•°æ®é›†æœ‰å‡ ä¸ªæ‚¨æƒ³è¦ç»„åˆçš„å­—æ®µï¼Œä¾‹å¦‚ï¼Œå¦‚æœæ•°æ®é›†æœ‰`question`å’Œ`answer`å­—æ®µå¹¶ä¸”æ‚¨æƒ³è¦ç»„åˆå®ƒä»¬ï¼Œæ‚¨å¯ä»¥å‘è®­ç»ƒå™¨ä¼ é€’ä¸€ä¸ªæ ¼å¼åŒ–å‡½æ•°æ¥å¤„ç†è¿™ä¸ªé—®é¢˜ã€‚ä¾‹å¦‚ï¼š

```python
def formatting_func(example):
    text = f"### Question: {example['question']}\n ### Answer: {example['answer']}"
    return text

training_args = SFTConfig(packing=True)
trainer = SFTTrainer(
    "facebook/opt-350m",
    train_dataset=dataset,
    args=training_args,
    formatting_func=formatting_func
)

trainer.train()
```

### å¯¹é¢„è®­ç»ƒæ¨¡å‹çš„æ§åˆ¶

æ‚¨å¯ä»¥ç›´æ¥å°†`from_pretrained()`æ–¹æ³•çš„kwargsä¼ é€’ç»™[`SFTConfig`]ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æƒ³ä»¥ä¸åŒç²¾åº¦åŠ è½½æ¨¡å‹ï¼Œç±»ä¼¼äº

```python
model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m", torch_dtype=torch.bfloat16)

...

training_args = SFTConfig(
    model_init_kwargs={
        "torch_dtype": "bfloat16",
    },
    output_dir="/tmp",
)
trainer = SFTTrainer(
    "facebook/opt-350m",
    train_dataset=dataset,
    args=training_args,
)

trainer.train()
```
è¯·æ³¨æ„ï¼Œæ”¯æŒ`from_pretrained()`çš„æ‰€æœ‰å…³é”®å­—å‚æ•°ã€‚

### è®­ç»ƒé€‚é…å™¨

æˆ‘ä»¬è¿˜æ”¯æŒä¸ğŸ¤— PEFTåº“çš„ç´§å¯†é›†æˆï¼Œä»¥ä¾¿ä»»ä½•ç”¨æˆ·éƒ½å¯ä»¥æ–¹ä¾¿åœ°è®­ç»ƒé€‚é…å™¨å¹¶åœ¨Hubä¸Šå…±äº«å®ƒä»¬ï¼Œè€Œä¸æ˜¯è®­ç»ƒæ•´ä¸ªæ¨¡å‹ã€‚

```python
from datasets import load_dataset
from trl import SFTConfig, SFTTrainer
from peft import LoraConfig

dataset = load_dataset("trl-lib/Capybara", split="train")

peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    target_modules="all-linear",
    modules_to_save=["lm_head", "embed_token"],
    task_type="CAUSAL_LM",
)

trainer = SFTTrainer(
    "Qwen/Qwen2.5-0.5B",
    train_dataset=dataset,
    args=SFTConfig(output_dir="Qwen2.5-0.5B-SFT"),
    peft_config=peft_config
)

trainer.train()
```

> [!WARNING]
> å¦‚æœèŠå¤©æ¨¡æ¿åŒ…å«ç‰¹æ®Šæ ‡è®°å¦‚`<|im_start|>`ï¼ˆChatMLï¼‰æˆ–`<|eot_id|>`ï¼ˆLlamaï¼‰ï¼ŒåµŒå…¥å±‚å’ŒLMå¤´å¿…é¡»é€šè¿‡`modules_to_save`å‚æ•°åŒ…å«åœ¨å¯è®­ç»ƒå‚æ•°ä¸­ã€‚æ²¡æœ‰è¿™ä¸ªï¼Œå¾®è°ƒçš„æ¨¡å‹å°†äº§ç”Ÿæ— ç•Œæˆ–æ— æ„ä¹‰çš„ç”Ÿæˆã€‚å¦‚æœèŠå¤©æ¨¡æ¿ä¸åŒ…å«ç‰¹æ®Šæ ‡è®°ï¼ˆä¾‹å¦‚Alpacaï¼‰ï¼Œé‚£ä¹ˆå¯ä»¥å¿½ç•¥`modules_to_save`å‚æ•°æˆ–è®¾ç½®ä¸º`None`ã€‚

æ‚¨ä¹Ÿå¯ä»¥ç»§ç»­è®­ç»ƒæ‚¨çš„`PeftModel`ã€‚ä¸ºæ­¤ï¼Œé¦–å…ˆåœ¨`SFTTrainer`å¤–éƒ¨åŠ è½½`PeftModel`ï¼Œå¹¶ç›´æ¥å°†å…¶ä¼ é€’ç»™è®­ç»ƒå™¨ï¼Œè€Œä¸ä¼ é€’`peft_config`å‚æ•°ã€‚

### ä½¿ç”¨åŸºç¡€8ä½æ¨¡å‹è®­ç»ƒé€‚é…å™¨

ä¸ºæ­¤ï¼Œæ‚¨éœ€è¦é¦–å…ˆåœ¨è®­ç»ƒå™¨å¤–éƒ¨åŠ è½½æ‚¨çš„8ä½æ¨¡å‹ï¼Œå¹¶å‘è®­ç»ƒå™¨ä¼ é€’`PeftConfig`ã€‚ä¾‹å¦‚ï¼š

```python
...

peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

model = AutoModelForCausalLM.from_pretrained(
    "EleutherAI/gpt-neo-125m",
    load_in_8bit=True,
    device_map="auto",
)

trainer = SFTTrainer(
    model,
    train_dataset=dataset,
    args=SFTConfig(),
    peft_config=peft_config,
)

trainer.train()
```

## ä½¿ç”¨Flash Attentionå’ŒFlash Attention 2

æ‚¨å¯ä»¥ä½¿ç”¨SFTTrainerå¼€ç®±å³ç”¨åœ°ä»Flash Attention 1å’Œ2ä¸­å—ç›Šï¼Œåªéœ€æœ€å°‘çš„ä»£ç æ›´æ”¹ã€‚
é¦–å…ˆï¼Œä¸ºäº†ç¡®ä¿æ‚¨æ‹¥æœ‰transformersçš„æ‰€æœ‰æœ€æ–°åŠŸèƒ½ï¼Œä»æºä»£ç å®‰è£…transformers

```bash
pip install -U git+https://github.com/huggingface/transformers.git
```

è¯·æ³¨æ„ï¼ŒFlash Attentionç°åœ¨åªåœ¨GPUä¸Šå·¥ä½œï¼Œå¹¶ä¸”åœ¨åŠç²¾åº¦æ¨¡å¼ä¸‹ï¼ˆå½“ä½¿ç”¨é€‚é…å™¨æ—¶ï¼ŒåŸºç¡€æ¨¡å‹ä»¥åŠç²¾åº¦åŠ è½½ï¼‰
è¿˜è¦æ³¨æ„ï¼Œè¿™ä¸¤ä¸ªåŠŸèƒ½ä¸å…¶ä»–å·¥å…·å¦‚é‡åŒ–å®Œå…¨å…¼å®¹ã€‚

### ä½¿ç”¨Flash-Attention 1

å¯¹äºFlash Attention 1ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨`BetterTransformer` APIå¹¶å¼ºåˆ¶è°ƒåº¦APIä½¿ç”¨Flash Attentionå†…æ ¸ã€‚é¦–å…ˆï¼Œå®‰è£…æœ€æ–°çš„optimumåŒ…ï¼š

```bash
pip install -U optimum
```

åŠ è½½æ¨¡å‹åï¼Œåœ¨`with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):`ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¸‹åŒ…è£…`trainer.train()`è°ƒç”¨ï¼š

```diff
...

+ with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):
    trainer.train()
```

è¯·æ³¨æ„ï¼Œæ‚¨ä¸èƒ½åœ¨ä»»æ„æ•°æ®é›†ä¸Šä½¿ç”¨Flash Attention 1è®­ç»ƒæ¨¡å‹ï¼Œå› ä¸ºå¦‚æœæ‚¨ä½¿ç”¨Flash Attentionå†…æ ¸ï¼Œ`torch.scaled_dot_product_attention`ä¸æ”¯æŒä½¿ç”¨å¡«å……æ ‡è®°è¿›è¡Œè®­ç»ƒã€‚å› æ­¤ï¼Œæ‚¨åªèƒ½åœ¨ä½¿ç”¨`packing=True`æ—¶ä½¿ç”¨è¯¥åŠŸèƒ½ã€‚å¦‚æœæ‚¨çš„æ•°æ®é›†åŒ…å«å¡«å……æ ‡è®°ï¼Œè¯·è€ƒè™‘åˆ‡æ¢åˆ°Flash Attention 2é›†æˆã€‚

ä»¥ä¸‹æ˜¯åœ¨å•ä¸ªNVIDIA-T4 16GBä¸Šä½¿ç”¨Flash Attention 1åœ¨é€Ÿåº¦å’Œå†…å­˜æ•ˆç‡æ–¹é¢å¯ä»¥è·å¾—çš„ä¸€äº›æ•°å­—ã€‚

| use_flash_attn_1 | model_name        | max_seq_len | batch_size | time per training step |
| ---------------- | ----------------- | ----------- | ---------- | ---------------------- |
| âœ“                | facebook/opt-350m | 2048        | 8          | ~59.1s                 |
|                  | facebook/opt-350m | 2048        | 8          | **OOM**                |
| âœ“                | facebook/opt-350m | 2048        | 4          | ~30.3s                 |
|                  | facebook/opt-350m | 2048        | 4          | ~148.9s                |

### ä½¿ç”¨Flash Attention-2

è¦ä½¿ç”¨Flash Attention 2ï¼Œé¦–å…ˆå®‰è£…æœ€æ–°çš„`flash-attn`åŒ…ï¼š

```bash
pip install -U flash-attn
```

å¹¶åœ¨è°ƒç”¨`from_pretrained`æ—¶æ·»åŠ `attn_implementation="flash_attention_2"`ï¼š

```python
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    load_in_4bit=True,
    attn_implementation="flash_attention_2"
)
```

å¦‚æœæ‚¨ä¸ä½¿ç”¨é‡åŒ–ï¼Œè¯·ç¡®ä¿æ‚¨çš„æ¨¡å‹ä»¥åŠç²¾åº¦åŠ è½½å¹¶è°ƒåº¦åˆ°æ”¯æŒçš„GPUè®¾å¤‡ä¸Šã€‚
åŠ è½½æ¨¡å‹åï¼Œæ‚¨å¯ä»¥æŒ‰åŸæ ·è®­ç»ƒå®ƒï¼Œæˆ–è€…åœ¨æ¨¡å‹è¢«é‡åŒ–çš„æƒ…å†µä¸‹é™„åŠ é€‚é…å™¨å¹¶åœ¨å…¶ä¸Šè®­ç»ƒé€‚é…å™¨ã€‚

ä¸Flash Attention 1ç›¸æ¯”ï¼Œé›†æˆä½¿å¾—åœ¨åŒ…å«å¡«å……æ ‡è®°çš„ä»»æ„æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹æˆä¸ºå¯èƒ½ã€‚

### ä½¿ç”¨æ¨¡å‹åˆ›å»ºå·¥å…·

æˆ‘ä»¬åŒ…å«äº†ä¸€ä¸ªåˆ›å»ºæ¨¡å‹çš„å·¥å…·å‡½æ•°ã€‚

[[autodoc]] ModelConfig

```python
from trl import ModelConfig, SFTTrainer, get_kbit_device_map, get_peft_config, get_quantization_config
model_args = ModelConfig(
    model_name_or_path="facebook/opt-350m"
    attn_implementation=None, # æˆ– "flash_attention_2"
)
torch_dtype = (
    model_args.torch_dtype
    if model_args.torch_dtype in ["auto", None]
    else getattr(torch, model_args.torch_dtype)
)
quantization_config = get_quantization_config(model_args)
model_kwargs = dict(
    revision=model_args.model_revision,
    trust_remote_code=model_args.trust_remote_code,
    attn_implementation=model_args.attn_implementation,
    torch_dtype=torch_dtype,
    use_cache=False if training_args.gradient_checkpointing else True,
    device_map=get_kbit_device_map() if quantization_config is not None else None,
    quantization_config=quantization_config,
)
model = AutoModelForCausalLM.from_pretrained(model_args.model_name_or_path, **model_kwargs)
trainer = SFTTrainer(
    ...,
    model=model_args.model_name_or_path,
    peft_config=get_peft_config(model_args),
)
```

### ä½¿ç”¨NEFTuneå¢å¼ºæ¨¡å‹æ€§èƒ½

NEFTuneæ˜¯ä¸€ç§æå‡èŠå¤©æ¨¡å‹æ€§èƒ½çš„æŠ€æœ¯ï¼Œç”±Jainç­‰äººçš„è®ºæ–‡["NEFTune: Noisy Embeddings Improve Instruction Finetuning"](https://huggingface.co/papers/2310.05914)æå‡ºã€‚å®ƒåŒ…æ‹¬åœ¨è®­ç»ƒæœŸé—´å‘åµŒå…¥å‘é‡æ·»åŠ å™ªå£°ã€‚æ ¹æ®è®ºæ–‡æ‘˜è¦ï¼š

> ä½¿ç”¨Alpacaå¯¹LLaMA-2-7Bè¿›è¡Œæ ‡å‡†å¾®è°ƒåœ¨AlpacaEvalä¸Šè¾¾åˆ°29.79%ï¼Œä½¿ç”¨å™ªå£°åµŒå…¥åä¸Šå‡åˆ°64.69%ã€‚NEFTuneåœ¨ç°ä»£æŒ‡ä»¤æ•°æ®é›†ä¸Šä¹Ÿä¼˜äºå¼ºåŸºçº¿ã€‚ä½¿ç”¨Evol-Instructè®­ç»ƒçš„æ¨¡å‹çœ‹åˆ°10%çš„æ”¹è¿›ï¼Œä½¿ç”¨ShareGPTçœ‹åˆ°8%çš„æ”¹è¿›ï¼Œä½¿ç”¨OpenPlatypusçœ‹åˆ°8%çš„æ”¹è¿›ã€‚å³ä½¿æ˜¯ç»è¿‡RLHFè¿›ä¸€æ­¥æ”¹è¿›çš„å¼ºå¤§æ¨¡å‹ï¼Œå¦‚LLaMA-2-Chatï¼Œä¹Ÿèƒ½ä»NEFTuneçš„é¢å¤–è®­ç»ƒä¸­å—ç›Šã€‚

<div style="text-align: center">
<img src="https://huggingface.co/datasets/trl-lib/documentation-images/resolve/main/neft-screenshot.png">
</div>

è¦åœ¨`SFTTrainer`ä¸­ä½¿ç”¨å®ƒï¼Œåªéœ€åœ¨åˆ›å»º`SFTConfig`å®ä¾‹æ—¶ä¼ é€’`neftune_noise_alpha`ã€‚è¯·æ³¨æ„ï¼Œä¸ºäº†é¿å…ä»»ä½•æ„å¤–è¡Œä¸ºï¼ŒNEFTuneåœ¨è®­ç»ƒåè¢«ç¦ç”¨ï¼Œä»¥æ¢å¤åˆ°åµŒå…¥å±‚çš„åŸå§‹è¡Œä¸ºã€‚

```python
from datasets import load_dataset
from trl import SFTConfig, SFTTrainer

dataset = load_dataset("stanfordnlp/imdb", split="train")

training_args = SFTConfig(
    neftune_noise_alpha=5,
)
trainer = SFTTrainer(
    "facebook/opt-350m",
    train_dataset=dataset,
    args=training_args,
)
trainer.train()
```

æˆ‘ä»¬é€šè¿‡åœ¨[OpenAssistantæ•°æ®é›†](https://huggingface.co/datasets/timdettmers/openassistant-guanaco)ä¸Šè®­ç»ƒ`mistralai/Mistral-7B-v0.1`æ¥æµ‹è¯•NEFTuneï¼Œå¹¶éªŒè¯ä½¿ç”¨NEFTuneåœ¨MT Benchä¸Šå¸¦æ¥äº†çº¦25%çš„æ€§èƒ½æå‡ã€‚

<div style="text-align: center">
<img src="https://huggingface.co/datasets/trl-lib/documentation-images/resolve/main/trl-neftune-mistral-7b.png">
</div>

ä½†è¯·æ³¨æ„ï¼Œæ€§èƒ½æå‡çš„æ•°é‡_å–å†³äºæ•°æ®é›†_ï¼Œç‰¹åˆ«æ˜¯åœ¨[UltraChat](https://huggingface.co/datasets/stingning/ultrachat)ç­‰åˆæˆæ•°æ®é›†ä¸Šåº”ç”¨NEFTuneé€šå¸¸ä¼šäº§ç”Ÿè¾ƒå°çš„æå‡ã€‚

### ä½¿ç”¨`unsloth`åŠ é€Ÿå¾®è°ƒ2å€

æ‚¨å¯ä»¥ä½¿ç”¨ä¸`SFTTrainer`å®Œå…¨å…¼å®¹çš„[`unsloth`](https://github.com/unslothai/unsloth)åº“è¿›ä¸€æ­¥åŠ é€ŸQLoRA / LoRAï¼ˆ2å€æ›´å¿«ï¼Œå†…å­˜å‡å°‘60%ï¼‰ã€‚ç›®å‰ï¼Œ`unsloth`ä»…æ”¯æŒLlamaï¼ˆYiã€TinyLlamaã€Qwenã€Deepseekç­‰ï¼‰å’ŒMistralæ¶æ„ã€‚ä¸‹é¢åˆ—å‡ºäº†1x A100ä¸Šçš„ä¸€äº›åŸºå‡†ï¼š

| 1 A100 40GB     | Dataset   | ğŸ¤—   | ğŸ¤— + Flash Attention 2 | ğŸ¦¥ Unsloth | ğŸ¦¥ VRAM saved |
| --------------- | --------- | --- | --------------------- | --------- | ------------ |
| Code Llama 34b  | Slim Orca | 1x  | 1.01x                 | **1.94x** | -22.7%       |
| Llama-2 7b      | Slim Orca | 1x  | 0.96x                 | **1.87x** | -39.3%       |
| Mistral 7b      | Slim Orca | 1x  | 1.17x                 | **1.88x** | -65.9%       |
| Tiny Llama 1.1b | Alpaca    | 1x  | 1.55x                 | **2.74x** | -57.8%       |

é¦–å…ˆï¼Œæ ¹æ®[å®˜æ–¹æ–‡æ¡£](https://github.com/unslothai/unsloth)å®‰è£…`unsloth`ã€‚å®‰è£…åï¼Œæ‚¨å¯ä»¥ä»¥éå¸¸ç®€å•çš„æ–¹å¼å°†unslothé›†æˆåˆ°æ‚¨çš„å·¥ä½œæµç¨‹ä¸­ï¼›æ‚¨åªéœ€è¦åŠ è½½`FastLanguageModel`è€Œä¸æ˜¯`AutoModelForCausalLM`ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
import torch
from trl import SFTConfig, SFTTrainer
from unsloth import FastLanguageModel

max_length = 2048 # æ”¯æŒè‡ªåŠ¨RoPEç¼©æ”¾ï¼Œæ‰€ä»¥é€‰æ‹©ä»»ä½•æ•°å­—

# åŠ è½½æ¨¡å‹
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/mistral-7b",
    max_seq_length=max_length,
    dtype=None,  # Noneç”¨äºè‡ªåŠ¨æ£€æµ‹ã€‚Float16ç”¨äºTesla T4ã€V100ï¼ŒBfloat16ç”¨äºAmpere+
    load_in_4bit=True,  # ä½¿ç”¨4ä½é‡åŒ–å‡å°‘å†…å­˜ä½¿ç”¨ã€‚å¯ä»¥æ˜¯False
    # token = "hf_...", # å¦‚æœä½¿ç”¨åƒmeta-llama/Llama-2-7b-hfè¿™æ ·çš„é—¨æ§æ¨¡å‹ï¼Œè¯·ä½¿ç”¨ä¸€ä¸ª
)

# è¿›è¡Œæ¨¡å‹ä¿®è¡¥å¹¶æ·»åŠ å¿«é€ŸLoRAæƒé‡
model = FastLanguageModel.get_peft_model(
    model,
    r=16,
    target_modules=[
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj",
    ],
    lora_alpha=16,
    lora_dropout=0,  # Dropout = 0ç›®å‰è¢«ä¼˜åŒ–
    bias="none",  # Bias = "none"ç›®å‰è¢«ä¼˜åŒ–
    use_gradient_checkpointing=True,
    random_state=3407,
)

training_args = SFTConfig(output_dir="./output", max_length=max_length)

trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
)
trainer.train()
```

ä¿å­˜çš„æ¨¡å‹ä¸Hugging Faceçš„transformersåº“å®Œå…¨å…¼å®¹ã€‚åœ¨[å®˜æ–¹ä»“åº“](https://github.com/unslothai/unsloth)ä¸­äº†è§£æ›´å¤šå…³äºunslothçš„ä¿¡æ¯ã€‚

## Liger-Kernelï¼šå¤šGPUè®­ç»ƒæé«˜20%ååé‡å¹¶å‡å°‘60%å†…å­˜

[Liger Kernel](https://github.com/linkedin/Liger-Kernel)æ˜¯ä¸“é—¨ä¸ºLLMè®­ç»ƒè®¾è®¡çš„Tritonå†…æ ¸é›†åˆã€‚å®ƒå¯ä»¥æœ‰æ•ˆåœ°å°†å¤šGPUè®­ç»ƒååé‡æé«˜20%ï¼Œå¹¶å°†å†…å­˜ä½¿ç”¨é‡å‡å°‘60%ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬å¯ä»¥**4å€**æˆ‘ä»¬çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œå¦‚ä¸‹é¢çš„åŸºå‡†æ‰€ç¤ºã€‚ä»–ä»¬å·²ç»å®ç°äº†Hugging Faceå…¼å®¹çš„`RMSNorm`ã€`RoPE`ã€`SwiGLU`ã€`CrossEntropy`ã€`FusedLinearCrossEntropy`ï¼Œè¿˜æœ‰æ›´å¤šå³å°†åˆ°æ¥ã€‚å†…æ ¸ä¸[Flash Attention](https://github.com/Dao-AILab/flash-attention)ã€[PyTorch FSDP](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html)å’Œ[Microsoft DeepSpeed](https://github.com/microsoft/DeepSpeed)å¼€ç®±å³ç”¨ã€‚

é€šè¿‡è¿™ç§å†…å­˜å‡å°‘ï¼Œæ‚¨å¯ä»¥æ½œåœ¨åœ°å…³é—­`cpu_offloading`æˆ–æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥è¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚

| Speed Up                 | Memory Reduction        |
|--------------------------|-------------------------|
| ![Speed up](https://raw.githubusercontent.com/linkedin/Liger-Kernel/main/docs/images/e2e-tps.png) | ![Memory](https://raw.githubusercontent.com/linkedin/Liger-Kernel/main/docs/images/e2e-memory.png) |

1. è¦åœ¨[`SFTTrainer`]ä¸­ä½¿ç”¨Liger-Kernelï¼Œé¦–å…ˆé€šè¿‡ä»¥ä¸‹æ–¹å¼å®‰è£…ï¼š

```bash
pip install liger-kernel
```

2. å®‰è£…åï¼Œåœ¨[`SFTConfig`]ä¸­è®¾ç½®`use_liger_kernel`ã€‚ä¸éœ€è¦å…¶ä»–æ›´æ”¹ï¼

```python
training_args = SFTConfig(
    use_liger_kernel=True,
    ...
)
```

è¦äº†è§£æ›´å¤šå…³äºLiger-Kernelçš„ä¿¡æ¯ï¼Œè¯·è®¿é—®ä»–ä»¬çš„[å®˜æ–¹ä»“åº“](https://github.com/linkedin/Liger-Kernel/)ã€‚

## æœ€ä½³å®è·µ

ä½¿ç”¨è¯¥è®­ç»ƒå™¨è®­ç»ƒæ¨¡å‹æ—¶ï¼Œè¯·æ³¨æ„ä»¥ä¸‹æœ€ä½³å®è·µï¼š

- [`SFTTrainer`]é»˜è®¤æ€»æ˜¯å°†åºåˆ—æˆªæ–­åˆ°[`SFTConfig`]çš„`max_length`å‚æ•°ã€‚å¦‚æœæ²¡æœ‰ä¼ é€’ï¼Œè®­ç»ƒå™¨å°†ä»åˆ†è¯å™¨æ£€ç´¢è¯¥å€¼ã€‚ä¸€äº›åˆ†è¯å™¨ä¸æä¾›é»˜è®¤å€¼ï¼Œæ‰€ä»¥æœ‰ä¸€ä¸ªæ£€æŸ¥æ¥æ£€ç´¢1024å’Œè¯¥å€¼ä¹‹é—´çš„æœ€å°å€¼ã€‚è®­ç»ƒå‰è¯·ç¡®ä¿æ£€æŸ¥å®ƒã€‚
- å¯¹äºåœ¨8ä½è®­ç»ƒé€‚é…å™¨ï¼Œæ‚¨å¯èƒ½éœ€è¦è°ƒæ•´PEFTçš„`prepare_model_for_kbit_training`æ–¹æ³•çš„å‚æ•°ï¼Œå› æ­¤æˆ‘ä»¬å»ºè®®ç”¨æˆ·ä½¿ç”¨`prepare_in_int8_kwargs`å­—æ®µï¼Œæˆ–åœ¨[`SFTTrainer`]å¤–éƒ¨åˆ›å»º`PeftModel`å¹¶ä¼ é€’å®ƒã€‚
- å¯¹äºä½¿ç”¨é€‚é…å™¨çš„æ›´å†…å­˜é«˜æ•ˆè®­ç»ƒï¼Œæ‚¨å¯ä»¥ä»¥8ä½åŠ è½½åŸºç¡€æ¨¡å‹ï¼Œä¸ºæ­¤åªéœ€åœ¨åˆ›å»º[`SFTTrainer`]æ—¶æ·»åŠ `load_in_8bit`å‚æ•°ï¼Œæˆ–åœ¨è®­ç»ƒå™¨å¤–éƒ¨ä»¥8ä½åˆ›å»ºåŸºç¡€æ¨¡å‹å¹¶ä¼ é€’å®ƒã€‚
- å¦‚æœæ‚¨åœ¨è®­ç»ƒå™¨å¤–éƒ¨åˆ›å»ºæ¨¡å‹ï¼Œè¯·ç¡®ä¿ä¸è¦å‘è®­ç»ƒå™¨ä¼ é€’ä»»ä½•ä¸`from_pretrained()`æ–¹æ³•ç›¸å…³çš„é¢å¤–å…³é”®å­—å‚æ•°ã€‚

## å¤šGPUè®­ç»ƒ

è®­ç»ƒå™¨ï¼ˆå› æ­¤SFTTrainerï¼‰æ”¯æŒå¤šGPUè®­ç»ƒã€‚å¦‚æœæ‚¨ä½¿ç”¨`python script.py`è¿è¡Œè„šæœ¬ï¼Œå®ƒå°†é»˜è®¤ä½¿ç”¨DPä½œä¸ºç­–ç•¥ï¼Œè¿™å¯èƒ½[æ¯”é¢„æœŸæ…¢](https://github.com/huggingface/trl/issues/1303)ã€‚è¦ä½¿ç”¨DDPï¼ˆé€šå¸¸æ¨èï¼Œæ›´å¤šä¿¡æ¯è¯·å‚è§[è¿™é‡Œ](https://huggingface.co/docs/transformers/en/perf_train_gpu_many?select-gpu=Accelerate#data-parallelism)ï¼‰ï¼Œæ‚¨å¿…é¡»ä½¿ç”¨`python -m torch.distributed.launch script.py`æˆ–`accelerate launch script.py`å¯åŠ¨è„šæœ¬ã€‚è¦ä½¿DDPå·¥ä½œï¼Œæ‚¨è¿˜å¿…é¡»æ£€æŸ¥ä»¥ä¸‹å†…å®¹ï¼š
- å¦‚æœæ‚¨ä½¿ç”¨gradient_checkpointingï¼Œè¯·åœ¨TrainingArgumentsä¸­æ·»åŠ ä»¥ä¸‹å†…å®¹ï¼š`gradient_checkpointing_kwargs={'use_reentrant':False}`ï¼ˆæ›´å¤šä¿¡æ¯è¯·å‚è§[è¿™é‡Œ](https://github.com/huggingface/transformers/issues/26969)
- ç¡®ä¿æ¨¡å‹æ”¾ç½®åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Šï¼š
```python
from accelerate import PartialState
device_string = PartialState().process_index
model = AutoModelForCausalLM.from_pretrained(
     ...
    device_map={'':device_string}
)
```

## GPTQè½¬æ¢

å®Œæˆè®­ç»ƒåï¼Œæ‚¨å¯èƒ½ä¼šé‡åˆ°GPTQé‡åŒ–çš„ä¸€äº›é—®é¢˜ã€‚å°†`gradient_accumulation_steps`é™ä½åˆ°`4`å°†è§£å†³é‡åŒ–åˆ°GPTQæ ¼å¼è¿‡ç¨‹ä¸­çš„å¤§å¤šæ•°é—®é¢˜ã€‚

## æ‰©å±•`SFTTrainer`ä»¥æ”¯æŒè§†è§‰è¯­è¨€æ¨¡å‹

`SFTTrainer`æœ¬èº«ä¸æ”¯æŒè§†è§‰è¯­è¨€æ•°æ®ã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬æä¾›äº†å¦‚ä½•è°ƒæ•´è®­ç»ƒå™¨ä»¥æ”¯æŒè§†è§‰è¯­è¨€æ•°æ®çš„æŒ‡å—ã€‚å…·ä½“æ¥è¯´ï¼Œæ‚¨éœ€è¦ä½¿ç”¨ä¸è§†è§‰è¯­è¨€æ•°æ®å…¼å®¹çš„è‡ªå®šä¹‰æ•°æ®æ•´ç†å™¨ã€‚æœ¬æŒ‡å—æ¦‚è¿°äº†è¿›è¡Œè¿™äº›è°ƒæ•´çš„æ­¥éª¤ã€‚å¯¹äºå…·ä½“ç¤ºä¾‹ï¼Œè¯·å‚è€ƒè„šæœ¬[`examples/scripts/sft_vlm.py`](https://github.com/huggingface/trl/blob/main/examples/scripts/sft_vlm.py)ï¼Œå®ƒæ¼”ç¤ºäº†å¦‚ä½•åœ¨[HuggingFaceH4/llava-instruct-mix-vsft](https://huggingface.co/datasets/HuggingFaceH4/llava-instruct-mix-vsft)æ•°æ®é›†ä¸Šå¾®è°ƒLLaVA 1.5æ¨¡å‹ã€‚

### å‡†å¤‡æ•°æ®

æ•°æ®æ ¼å¼æ˜¯çµæ´»çš„ï¼Œåªè¦å®ƒä¸æˆ‘ä»¬ç¨åå°†å®šä¹‰çš„è‡ªå®šä¹‰æ•´ç†å™¨å…¼å®¹ã€‚å¸¸è§çš„æ–¹æ³•æ˜¯ä½¿ç”¨å¯¹è¯æ•°æ®ã€‚ç”±äºæ•°æ®åŒ…æ‹¬æ–‡æœ¬å’Œå›¾åƒï¼Œæ ¼å¼éœ€è¦ç›¸åº”è°ƒæ•´ã€‚ä»¥ä¸‹æ˜¯æ¶‰åŠæ–‡æœ¬å’Œå›¾åƒçš„å¯¹è¯æ•°æ®æ ¼å¼ç¤ºä¾‹ï¼š

```python
images = ["obama.png"]
messages = [
    {
        "role": "user",
        "content": [
            {"type": "text", "text": "Who is this?"},
            {"type": "image"}
        ]
    },
    {
        "role": "assistant",
        "content": [
            {"type": "text", "text": "Barack Obama"}
        ]
    },
    {
        "role": "user",
        "content": [
            {"type": "text", "text": "What is he famous for?"}
        ]
    },
    {
        "role": "assistant",
        "content": [
            {"type": "text", "text": "He is the 44th President of the United States."}
        ]
    }
]
```

ä¸ºäº†è¯´æ˜å¦‚ä½•ä½¿ç”¨LLaVAæ¨¡å‹å¤„ç†è¿™ç§æ•°æ®æ ¼å¼ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç ï¼š

```python
from transformers import AutoProcessor

processor = AutoProcessor.from_pretrained("llava-hf/llava-1.5-7b-hf")
print(processor.apply_chat_template(messages, tokenize=False))
```

è¾“å‡ºå°†æ ¼å¼åŒ–ä¸ºï¼š

```txt
Who is this? ASSISTANT: Barack Obama USER: What is he famous for? ASSISTANT: He is the 44th President of the United States. 
```

<iframe src="https://huggingface.co/datasets/HuggingFaceH4/llava-instruct-mix-vsft/embed/viewer/default/train" frameborder="0" width="100%" height="560px"></iframe>

### å¤„ç†å¤šæ¨¡æ€æ•°æ®çš„è‡ªå®šä¹‰æ•´ç†å™¨

ä¸`SFTTrainer`çš„é»˜è®¤è¡Œä¸ºä¸åŒï¼Œå¤„ç†å¤šæ¨¡æ€æ•°æ®æ˜¯åœ¨æ•°æ®æ•´ç†è¿‡ç¨‹ä¸­åŠ¨æ€å®Œæˆçš„ã€‚ä¸ºæ­¤ï¼Œæ‚¨éœ€è¦å®šä¹‰ä¸€ä¸ªå¤„ç†æ–‡æœ¬å’Œå›¾åƒçš„è‡ªå®šä¹‰æ•´ç†å™¨ã€‚æ­¤æ•´ç†å™¨å¿…é¡»å°†ç¤ºä¾‹åˆ—è¡¨ä½œä¸ºè¾“å…¥ï¼ˆæœ‰å…³æ•°æ®æ ¼å¼çš„ç¤ºä¾‹ï¼Œè¯·å‚è§ä¸Šä¸€èŠ‚ï¼‰ï¼Œå¹¶è¿”å›ä¸€æ‰¹å¤„ç†åçš„æ•°æ®ã€‚ä»¥ä¸‹æ˜¯æ­¤ç±»æ•´ç†å™¨çš„ç¤ºä¾‹ï¼š

```python
def collate_fn(examples):
    # è·å–æ–‡æœ¬å’Œå›¾åƒï¼Œå¹¶åº”ç”¨èŠå¤©æ¨¡æ¿
    texts = [processor.apply_chat_template(example["messages"], tokenize=False) for example in examples]
    images = [example["images"][0] for example in examples]

    # å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯å¹¶å¤„ç†å›¾åƒ
    batch = processor(texts, images, return_tensors="pt", padding=True)

    # æ ‡ç­¾æ˜¯input_idsï¼Œæˆ‘ä»¬åœ¨æŸå¤±è®¡ç®—ä¸­å±è”½å¡«å……æ ‡è®°
    labels = batch["input_ids"].clone()
    labels[labels == processor.tokenizer.pad_token_id] = -100
    batch["labels"] = labels

    return batch
```

æˆ‘ä»¬å¯ä»¥é€šè¿‡è¿è¡Œä»¥ä¸‹ä»£ç éªŒè¯æ•´ç†å™¨æŒ‰é¢„æœŸå·¥ä½œï¼š

```python
from datasets import load_dataset

dataset = load_dataset("HuggingFaceH4/llava-instruct-mix-vsft", split="train")
examples = [dataset[0], dataset[1]]  # ä»…ä¸ºäº†ç¤ºä¾‹çš„ä¸¤ä¸ªç¤ºä¾‹
collated_data = collate_fn(examples)
print(collated_data.keys())  # dict_keys(['input_ids', 'attention_mask', 'pixel_values', 'labels'])
```

### è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹

ç°åœ¨æˆ‘ä»¬å·²ç»å‡†å¤‡äº†æ•°æ®å¹¶å®šä¹‰äº†æ•´ç†å™¨ï¼Œæˆ‘ä»¬å¯ä»¥ç»§ç»­è®­ç»ƒæ¨¡å‹ã€‚ä¸ºäº†ç¡®ä¿æ•°æ®ä¸è¢«å¤„ç†ä¸ºä»…æ–‡æœ¬ï¼Œæˆ‘ä»¬éœ€è¦åœ¨`SFTConfig`ä¸­è®¾ç½®å‡ ä¸ªå‚æ•°ï¼Œç‰¹åˆ«æ˜¯`remove_unused_columns`å’Œ`skip_prepare_dataset`ä¸º`True`ï¼Œä»¥é¿å…æ•°æ®é›†çš„é»˜è®¤å¤„ç†ã€‚ä»¥ä¸‹æ˜¯å¦‚ä½•è®¾ç½®`SFTTrainer`çš„ç¤ºä¾‹ã€‚

```python
training_args.remove_unused_columns = False
training_args.dataset_kwargs = {"skip_prepare_dataset": True}

trainer = SFTTrainer(
    model=model,
    args=training_args,
    data_collator=collate_fn,
    train_dataset=train_dataset,
    processing_class=processor.tokenizer,
)
```

åœ¨[HuggingFaceH4/llava-instruct-mix-vsft](https://huggingface.co/datasets/HuggingFaceH4/llava-instruct-mix-vsft)æ•°æ®é›†ä¸Šè®­ç»ƒLLaVa 1.5çš„å®Œæ•´ç¤ºä¾‹å¯ä»¥åœ¨è„šæœ¬[`examples/scripts/sft_vlm.py`](https://github.com/huggingface/trl/blob/main/examples/scripts/sft_vlm.py)ä¸­æ‰¾åˆ°ã€‚

- [å®éªŒè·Ÿè¸ª](https://wandb.ai/huggingface/trl/runs/2b2c5l7s)
- [è®­ç»ƒæ¨¡å‹](https://huggingface.co/HuggingFaceH4/sft-llava-1.5-7b-hf)

## SFTTrainer

[[autodoc]] SFTTrainer

## SFTConfig

[[autodoc]] SFTConfig

## æ•°æ®é›†

åœ¨SFTTrainerä¸­ï¼Œæˆ‘ä»¬æ™ºèƒ½åœ°æ”¯æŒ`datasets.IterableDataset`ä»¥åŠå…¶ä»–æ ·å¼çš„æ•°æ®é›†ã€‚å¦‚æœæ‚¨ä½¿ç”¨ä¸æƒ³å…¨éƒ¨ä¿å­˜åˆ°ç£ç›˜çš„å¤§å‹è¯­æ–™åº“ï¼Œè¿™å¾ˆæœ‰ç”¨ã€‚æ•°æ®å°†åœ¨é£è¡Œä¸­è¿›è¡Œåˆ†è¯å’Œå¤„ç†ï¼Œå³ä½¿å¯ç”¨äº†æ‰“åŒ…ã€‚

æ­¤å¤–ï¼Œåœ¨SFTTrainerä¸­ï¼Œå¦‚æœå®ƒä»¬æ˜¯`datasets.Dataset`æˆ–`datasets.IterableDataset`ï¼Œæˆ‘ä»¬æ”¯æŒé¢„åˆ†è¯æ•°æ®é›†ã€‚æ¢å¥è¯è¯´ï¼Œå¦‚æœè¿™æ ·çš„æ•°æ®é›†æœ‰`input_ids`åˆ—ï¼Œå°†ä¸ä¼šè¿›è¡Œè¿›ä¸€æ­¥çš„å¤„ç†ï¼ˆåˆ†è¯æˆ–æ‰“åŒ…ï¼‰ï¼Œæ•°æ®é›†å°†æŒ‰åŸæ ·ä½¿ç”¨ã€‚å¦‚æœæ‚¨åœ¨æ­¤è„šæœ¬ä¹‹å¤–é¢„åˆ†è¯äº†æ•°æ®é›†å¹¶æƒ³è¦ç›´æ¥é‡ç”¨å®ƒï¼Œè¿™å¯èƒ½å¾ˆæœ‰ç”¨ã€‚ 