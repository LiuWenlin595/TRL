# DPO è®­ç»ƒå™¨

[![](https://img.shields.io/badge/All_models-DPO-blue)](https://huggingface.co/models?other=dpo,trl) [![](https://img.shields.io/badge/smol_course-Chapter_2-yellow)](https://github.com/huggingface/smol-course/tree/main/2_preference_alignment)

## æ¦‚è¿°

TRL æ”¯æŒ DPO è®­ç»ƒå™¨ï¼Œç”¨äºä»åå¥½æ•°æ®è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œå¦‚ [Rafael Rafailov](https://huggingface.co/rmrafailov)ã€Archit Sharmaã€Eric Mitchellã€[Stefano Ermon](https://huggingface.co/ermonste)ã€[Christopher D. Manning](https://huggingface.co/manning)ã€[Chelsea Finn](https://huggingface.co/cbfinn) åœ¨è®ºæ–‡ [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://huggingface.co/papers/2305.18290) ä¸­æ‰€è¿°ã€‚

è®ºæ–‡æ‘˜è¦å¦‚ä¸‹ï¼š

> è™½ç„¶å¤§è§„æ¨¡æ— ç›‘ç£è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰å­¦ä¹ å¹¿æ³›çš„ä¸–ç•ŒçŸ¥è¯†å’Œä¸€äº›æ¨ç†æŠ€èƒ½ï¼Œä½†ç”±äºå…¶è®­ç»ƒçš„å®Œå…¨æ— ç›‘ç£æ€§è´¨ï¼Œå®ç°å¯¹å…¶è¡Œä¸ºçš„ç²¾ç¡®æ§åˆ¶æ˜¯å›°éš¾çš„ã€‚è·å¾—è¿™ç§å¯æ“æ§æ€§çš„ç°æœ‰æ–¹æ³•æ”¶é›†æ¨¡å‹ç”Ÿæˆç›¸å¯¹è´¨é‡çš„äººå·¥æ ‡ç­¾ï¼Œå¹¶å¾®è°ƒæ— ç›‘ç£ LM ä»¥ä¸è¿™äº›åå¥½ä¿æŒä¸€è‡´ï¼Œé€šå¸¸é€šè¿‡äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ã€‚ç„¶è€Œï¼ŒRLHF æ˜¯ä¸€ä¸ªå¤æ‚ä¸”é€šå¸¸ä¸ç¨³å®šçš„è¿‡ç¨‹ï¼Œé¦–å…ˆæ‹Ÿåˆåæ˜ äººç±»åå¥½çš„å¥–åŠ±æ¨¡å‹ï¼Œç„¶åä½¿ç”¨å¼ºåŒ–å­¦ä¹ å¾®è°ƒå¤§å‹æ— ç›‘ç£ LM ä»¥æœ€å¤§åŒ–è¿™ä¸ªä¼°è®¡çš„å¥–åŠ±ï¼Œè€Œä¸ä¼šåç¦»åŸå§‹æ¨¡å‹å¤ªè¿œã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº† RLHF ä¸­å¥–åŠ±æ¨¡å‹çš„æ–°å‚æ•°åŒ–ï¼Œå®ƒèƒ½å¤Ÿä»¥å°é—­å½¢å¼æå–ç›¸åº”çš„æœ€ä¼˜ç­–ç•¥ï¼Œå…è®¸æˆ‘ä»¬ä»…ä½¿ç”¨ç®€å•çš„åˆ†ç±»æŸå¤±æ¥è§£å†³æ ‡å‡† RLHF é—®é¢˜ã€‚æˆ‘ä»¬ç§°ä¹‹ä¸ºç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰çš„ç®—æ³•æ˜¯ç¨³å®šçš„ã€é«˜æ€§èƒ½çš„ä¸”è®¡ç®—è½»é‡çº§çš„ï¼Œæ¶ˆé™¤äº†åœ¨å¾®è°ƒæœŸé—´ä» LM é‡‡æ ·æˆ–æ‰§è¡Œé‡è¦è¶…å‚æ•°è°ƒä¼˜çš„éœ€è¦ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒDPO å¯ä»¥å¾®è°ƒ LM ä»¥ä¸äººç±»åå¥½ä¿æŒä¸€è‡´ï¼Œæ•ˆæœä¸ç°æœ‰æ–¹æ³•ç›¸å½“æˆ–æ›´å¥½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä½¿ç”¨ DPO å¾®è°ƒåœ¨æ§åˆ¶ç”Ÿæˆæƒ…æ„Ÿçš„èƒ½åŠ›æ–¹é¢è¶…è¿‡äº†åŸºäº PPO çš„ RLHFï¼Œåœ¨æ‘˜è¦å’Œå•è½®å¯¹è¯ä¸­åŒ¹é…æˆ–æ”¹è¿›äº†å“åº”è´¨é‡ï¼ŒåŒæ—¶å®ç°å’Œè®­ç»ƒèµ·æ¥è¦ç®€å•å¾—å¤šã€‚

ã€importantã€‘
ç¬¬ä¸€æ­¥æ˜¯è®­ç»ƒ SFT æ¨¡å‹ï¼Œä»¥ç¡®ä¿æˆ‘ä»¬è®­ç»ƒçš„æ•°æ®å¯¹ DPO ç®—æ³•æ¥è¯´æ˜¯åˆ†å¸ƒå†…çš„ã€‚

ç„¶åï¼Œé€šè¿‡ DPO å¾®è°ƒè¯­è¨€æ¨¡å‹åŒ…æ‹¬ä¸¤ä¸ªæ­¥éª¤ï¼Œæ¯” [PPO](ppo_trainer) æ›´å®¹æ˜“ï¼š

1. **æ•°æ®æ”¶é›†**ï¼šæ”¶é›† [åå¥½æ•°æ®é›†](dataset_formats#preference)ï¼ŒåŒ…å«ç»™å®šæç¤ºçš„æ­£é¢å’Œè´Ÿé¢é€‰æ‹©çš„ç”Ÿæˆå¯¹ã€‚
2. **ä¼˜åŒ–**ï¼šç›´æ¥æœ€å¤§åŒ– DPO æŸå¤±çš„å¯¹æ•°ä¼¼ç„¶ã€‚

è¿™ä¸ªè¿‡ç¨‹åœ¨ä¸‹é¢çš„ç¤ºæ„å›¾ä¸­è¯´æ˜ï¼ˆæ¥è‡ª [DPO è®ºæ–‡çš„å›¾ 1](https://huggingface.co/papers/2305.18290)ï¼‰ï¼š

![](https://github.com/huggingface/trl/assets/49240599/9150fac6-3d88-4ca2-8ec6-2a6f3473216d)

åœ¨ [åŸå§‹è®ºæ–‡](https://huggingface.co/papers/2305.18290) ä¸­äº†è§£æ›´å¤šå…³äº DPO ç®—æ³•çš„ä¿¡æ¯ã€‚

## å¿«é€Ÿå¼€å§‹

è¿™ä¸ªç¤ºä¾‹æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨ DPO æ–¹æ³•è®­ç»ƒæ¨¡å‹ã€‚æˆ‘ä»¬ä½¿ç”¨ [Qwen 0.5B æ¨¡å‹](https://huggingface.co/Qwen/Qwen2-0.5B-Instruct) ä½œä¸ºåŸºç¡€æ¨¡å‹ã€‚æˆ‘ä»¬ä½¿ç”¨æ¥è‡ª [UltraFeedback æ•°æ®é›†](https://huggingface.co/datasets/openbmb/UltraFeedback) çš„åå¥½æ•°æ®ã€‚æ‚¨å¯ä»¥åœ¨è¿™é‡ŒæŸ¥çœ‹æ•°æ®é›†ä¸­çš„æ•°æ®ï¼š

<iframe
  src="https://huggingface.co/datasets/trl-lib/ultrafeedback_binarized/embed/viewer/default/train?row=0"
  frameborder="0"
  width="100%"
  height="560px"
></iframe>

ä»¥ä¸‹æ˜¯è®­ç»ƒæ¨¡å‹çš„è„šæœ¬ï¼š

```python
# train_dpo.py
from datasets import load_dataset
from trl import DPOConfig, DPOTrainer
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2-0.5B-Instruct")
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2-0.5B-Instruct")
train_dataset = load_dataset("trl-lib/ultrafeedback_binarized", split="train")

training_args = DPOConfig(output_dir="Qwen2-0.5B-DPO", logging_steps=10)
trainer = DPOTrainer(model=model, args=training_args, processing_class=tokenizer, train_dataset=train_dataset)
trainer.train()
```

ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ‰§è¡Œè„šæœ¬ï¼š

```bash
accelerate launch train_dpo.py
```

åœ¨ 8 ä¸ª GPU ä¸Šåˆ†å¸ƒå¼è®­ç»ƒå¤§çº¦éœ€è¦ 3 åˆ†é’Ÿã€‚æ‚¨å¯ä»¥é€šè¿‡æ£€æŸ¥å¥–åŠ±å›¾æ¥éªŒè¯è®­ç»ƒè¿›åº¦ã€‚å¥–åŠ±è¾¹ç•Œçš„ä¸Šå‡è¶‹åŠ¿è¡¨æ˜æ¨¡å‹æ­£åœ¨æ”¹è¿›ï¼Œå¹¶éšç€æ—¶é—´çš„æ¨ç§»ç”Ÿæˆæ›´å¥½çš„å“åº”ã€‚

![](https://huggingface.co/datasets/trl-lib/documentation-images/resolve/main/dpo-qwen2-reward-margin.png)

è¦æŸ¥çœ‹ [è®­ç»ƒå¥½çš„æ¨¡å‹](https://huggingface.co/trl-lib/Qwen2-0.5B-DPO) çš„è¡¨ç°ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ [Transformers Chat CLI](https://huggingface.co/docs/transformers/quicktour#chat-with-text-generation-models)ã€‚

<pre><code>$ transformers chat trl-lib/Qwen2-0.5B-DPO
<strong><span style="color: red;">&lt;shirin_yamani&gt;:</span></strong>
What is Huggingface?

<strong><span style="color: blue;">&lt;trl-lib/Qwen2-0.5B-DPO&gt;:</span></strong>
Huggingface is a platform that allows users to access a variety of open-source machine learning resources such as pre-trained models and datasets Huggingface is a platform that allows users to access a variety of open-source machine learning resources such as pre-trained models and datasets for the development of machine learning models and applications. It provides a repository of over 300, 000 pre-trained models in  Huggingface is a platform that allows users to access a variety of open-source machine learning resources such as pre-trained models and datasets for the development of machine learning models and applications. It provides a repository of over 300, 000  pre-trained models in a variety of languages, enabling users to explore and utilize the latest techniques and technologies in the field of machine learning.
</code></pre>

## é¢„æœŸæ•°æ®é›†ç±»å‹

DPO éœ€è¦ [åå¥½æ•°æ®é›†](dataset_formats#preference)ã€‚[`DPOTrainer`] æ”¯æŒ [å¯¹è¯å¼](dataset_formats#conversational) å’Œ [æ ‡å‡†](dataset_formats#standard) æ•°æ®é›†æ ¼å¼ã€‚å½“æä¾›å¯¹è¯å¼æ•°æ®é›†æ—¶ï¼Œè®­ç»ƒå™¨å°†è‡ªåŠ¨å¯¹æ•°æ®é›†åº”ç”¨èŠå¤©æ¨¡æ¿ã€‚

è™½ç„¶ [`DPOTrainer`] æ”¯æŒæ˜¾å¼å’Œéšå¼æç¤ºï¼Œä½†æˆ‘ä»¬å»ºè®®ä½¿ç”¨æ˜¾å¼æç¤ºã€‚å¦‚æœæä¾›éšå¼æç¤ºæ•°æ®é›†ï¼Œè®­ç»ƒå™¨å°†è‡ªåŠ¨ä» `"chosen"` å’Œ `"rejected"` åˆ—ä¸­æå–æç¤ºã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜… [åå¥½æ ·å¼](dataset_formats#preference) éƒ¨åˆ†ã€‚

### è§†è§‰è¯­è¨€æ¨¡å‹çš„ç‰¹æ®Šè€ƒè™‘

[`DPOTrainer`] æ”¯æŒå¾®è°ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ã€‚å¯¹äºè¿™äº›æ¨¡å‹ï¼Œéœ€è¦è§†è§‰æ•°æ®é›†ã€‚è¦äº†è§£è§†è§‰æ•°æ®é›†çš„å…·ä½“æ ¼å¼ï¼Œè¯·å‚é˜… [è§†è§‰æ•°æ®é›†æ ¼å¼](dataset_formats#vision-datasets) éƒ¨åˆ†ã€‚

æ­¤å¤–ï¼Œä¸ä½¿ç”¨ `tokenizer` çš„æ ‡å‡†åŸºäºæ–‡æœ¬çš„æ¨¡å‹ä¸åŒï¼Œå¯¹äº VLMï¼Œæ‚¨åº”è¯¥ç”¨ `processor` æ›¿æ¢ `tokenizer`ã€‚

```diff
- model = AutoModelForCausalLM.from_pretrained(model_id)
+ model = AutoModelForVision2Seq.from_pretrained(model_id)

- tokenizer = AutoTokenizer.from_pretrained(model_id)
+ processor = AutoProcessor.from_pretrained(model_id)

  trainer = DPOTrainer(
      model,
      args=training_args,
      train_dataset=train_dataset,
-     processing_class=tokenizer,
+     processing_class=processor,
)
```

æœ‰å…³å¾®è°ƒè§†è§‰è¯­è¨€æ¨¡å‹çš„å®Œæ•´ç¤ºä¾‹ï¼Œè¯·å‚é˜… [`examples/scripts/dpo_vlm.py`](https://github.com/huggingface/trl/blob/main/examples/scripts/dpo_vlm.py) ä¸­çš„è„šæœ¬ã€‚

## ç¤ºä¾‹è„šæœ¬

æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªä½¿ç”¨ DPO æ–¹æ³•è®­ç»ƒæ¨¡å‹çš„ç¤ºä¾‹è„šæœ¬ã€‚è„šæœ¬åœ¨ [`trl/scripts/dpo.py`](https://github.com/huggingface/trl/blob/main/trl/scripts/dpo.py) ä¸­å¯ç”¨

è¦åœ¨ [UltraFeedback æ•°æ®é›†](https://huggingface.co/datasets/trl-lib/ultrafeedback_binarized) ä¸Šä½¿ç”¨ [Qwen2 0.5B æ¨¡å‹](https://huggingface.co/Qwen/Qwen2-0.5B-Instruct) æµ‹è¯• DPO è„šæœ¬ï¼Œè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```bash
accelerate launch trl/scripts/dpo.py \
    --model_name_or_path Qwen/Qwen2-0.5B-Instruct \
    --dataset_name trl-lib/ultrafeedback_binarized \
    --num_train_epochs 1 \
    --logging_steps 25 \
    --output_dir Qwen2-0.5B-DPO
```

## è®°å½•çš„æŒ‡æ ‡

åœ¨è®­ç»ƒå’Œè¯„ä¼°æœŸé—´ï¼Œæˆ‘ä»¬è®°å½•ä»¥ä¸‹å¥–åŠ±æŒ‡æ ‡ï¼š

- `rewards/chosen`ï¼šç­–ç•¥æ¨¡å‹å’Œå‚è€ƒæ¨¡å‹å¯¹æ‰€é€‰å“åº”çš„å¯¹æ•°æ¦‚ç‡å·®å¼‚çš„å¹³å‡å€¼ï¼ŒæŒ‰ beta ç¼©æ”¾
- `rewards/rejected`ï¼šç­–ç•¥æ¨¡å‹å’Œå‚è€ƒæ¨¡å‹å¯¹è¢«æ‹’ç»å“åº”çš„å¯¹æ•°æ¦‚ç‡å·®å¼‚çš„å¹³å‡å€¼ï¼ŒæŒ‰ beta ç¼©æ”¾
- `rewards/accuracies`ï¼šæ‰€é€‰å¥–åŠ±å¤§äºç›¸åº”è¢«æ‹’ç»å¥–åŠ±çš„å¹³å‡é¢‘ç‡
- `rewards/margins`ï¼šæ‰€é€‰å¥–åŠ±ä¸ç›¸åº”è¢«æ‹’ç»å¥–åŠ±ä¹‹é—´çš„å¹³å‡å·®å¼‚

## æŸå¤±å‡½æ•°

DPO ç®—æ³•æ”¯æŒå¤šç§æŸå¤±å‡½æ•°ã€‚æŸå¤±å‡½æ•°å¯ä»¥ä½¿ç”¨ [`DPOConfig`] ä¸­çš„ `loss_type` å‚æ•°è®¾ç½®ã€‚æ”¯æŒä»¥ä¸‹æŸå¤±å‡½æ•°ï¼š

| `loss_type=`                           | æè¿°                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| -------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `"sigmoid"` (é»˜è®¤)                  | ç»™å®šåå¥½æ•°æ®ï¼Œæˆ‘ä»¬å¯ä»¥æ ¹æ® Bradley-Terry æ¨¡å‹æ‹ŸåˆäºŒå…ƒåˆ†ç±»å™¨ï¼Œå®é™…ä¸Š [DPO](https://huggingface.co/papers/2305.18290) ä½œè€…æå‡ºäº†é€šè¿‡ `logsigmoid` å¯¹å½’ä¸€åŒ–ä¼¼ç„¶ä½¿ç”¨ sigmoid æŸå¤±æ¥æ‹Ÿåˆé€»è¾‘å›å½’ã€‚                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| `"hinge"`                              | [RSO](https://huggingface.co/papers/2309.06657) ä½œè€…æå‡ºåœ¨ [SLiC](https://huggingface.co/papers/2305.10425) è®ºæ–‡çš„å½’ä¸€åŒ–ä¼¼ç„¶ä¸Šä½¿ç”¨é“°é“¾æŸå¤±ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ`beta` æ˜¯è¾¹ç•Œçš„å€’æ•°ã€‚                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| `"ipo"`                                | [IPO](https://huggingface.co/papers/2310.12036) ä½œè€…æä¾›äº†å¯¹ DPO ç®—æ³•çš„æ›´æ·±å±‚æ¬¡ç†è®ºç†è§£ï¼Œè¯†åˆ«äº†è¿‡æ‹Ÿåˆé—®é¢˜å¹¶æå‡ºäº†æ›¿ä»£æŸå¤±ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ`beta` æ˜¯æ‰€é€‰ä¸è¢«æ‹’ç»å®Œæˆå¯¹çš„å¯¹æ•°ä¼¼ç„¶æ¯”ä¹‹é—´å·®è·çš„å€’æ•°ï¼Œå› æ­¤ `beta` è¶Šå°ï¼Œè¿™ä¸ªå·®è·è¶Šå¤§ã€‚æ ¹æ®è®ºæ–‡ï¼ŒæŸå¤±åœ¨å®Œæˆçš„å¯¹æ•°ä¼¼ç„¶ä¸Šå¹³å‡ï¼ˆä¸ä»…æ±‚å’Œçš„ DPO ä¸åŒï¼‰ã€‚                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| `"exo_pair"`                           | [EXO](https://huggingface.co/papers/2402.00856) ä½œè€…æå‡ºæœ€å°åŒ–åå‘ KL è€Œä¸æ˜¯ DPO çš„è´Ÿå¯¹æ•° sigmoid æŸå¤±ï¼ˆå¯¹åº”äºå‰å‘ KLï¼‰ã€‚è®¾ç½®éé›¶ `label_smoothing`ï¼ˆé»˜è®¤ `1e-3`ï¼‰ä¼šå¯¼è‡´æˆå¯¹åå¥½ä¸Š EXO çš„ç®€åŒ–ç‰ˆæœ¬ï¼ˆå‚è§ [EXO è®ºæ–‡](https://huggingface.co/papers/2402.00856) çš„ç­‰å¼ (16)ï¼‰ã€‚EXO çš„å®Œæ•´ç‰ˆæœ¬ä½¿ç”¨ SFT ç­–ç•¥ç”Ÿæˆçš„ `K>2` ä¸ªå®Œæˆï¼Œå½“ `K` è¶³å¤Ÿå¤§æ—¶ï¼Œè¿™æˆä¸º PPO ç›®æ ‡çš„æ— åä¼°è®¡å™¨ï¼ˆç›´åˆ°å¸¸æ•°ï¼‰ã€‚                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| `"nca_pair"`                           | [NCA](https://huggingface.co/papers/2402.05369) ä½œè€…è¡¨æ˜ NCA ä¼˜åŒ–æ¯ä¸ªå“åº”çš„ç»å¯¹ä¼¼ç„¶è€Œä¸æ˜¯ç›¸å¯¹ä¼¼ç„¶ã€‚                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| `"robust"`                             | [Robust DPO](https://huggingface.co/papers/2403.00409) ä½œè€…æå‡ºäº†å¯¹æ•°æ®ä¸­åå¥½å™ªå£°å…·æœ‰é²æ£’æ€§çš„ DPO æŸå¤±çš„æ— åä¼°è®¡ã€‚ä¸ cDPO ä¸€æ ·ï¼Œå®ƒå‡è®¾åå¥½æ ‡ç­¾ä»¥æŸç§æ¦‚ç‡æœ‰å™ªå£°ã€‚åœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œ[`DPOConfig`] ä¸­çš„ `label_smoothing` å‚æ•°ç”¨äºå»ºæ¨¡ç°æœ‰æ ‡ç­¾å™ªå£°çš„æ¦‚ç‡ã€‚è¦åº”ç”¨è¿™ç§ä¿å®ˆæŸå¤±ï¼Œè¯·å°† `label_smoothing` è®¾ç½®ä¸ºå¤§äº 0.0 çš„å€¼ï¼ˆåœ¨ 0.0 å’Œ 0.5 ä¹‹é—´ï¼›é»˜è®¤å€¼ä¸º 0.0ï¼‰                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| `"bco_pair"`                           | [BCO](https://huggingface.co/papers/2404.04656) ä½œè€…è®­ç»ƒä¸€ä¸ªäºŒå…ƒåˆ†ç±»å™¨ï¼Œå…¶ logit ä½œä¸ºå¥–åŠ±ï¼Œä½¿å¾—åˆ†ç±»å™¨å°† {æç¤ºï¼Œæ‰€é€‰å®Œæˆ} å¯¹æ˜ å°„åˆ° 1ï¼Œå°† {æç¤ºï¼Œè¢«æ‹’ç»å®Œæˆ} å¯¹æ˜ å°„åˆ° 0ã€‚å¯¹äºæœªé…å¯¹æ•°æ®ï¼Œæˆ‘ä»¬æ¨èä¸“ç”¨çš„ [`BCOTrainer`]ã€‚                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| `"sppo_hard"`                          | [SPPO](https://huggingface.co/papers/2405.00675) ä½œè€…å£°ç§° SPPO èƒ½å¤Ÿé€šè¿‡å°†æ‰€é€‰å¥–åŠ±æ¨è‡³æœ€å¤§ 1/2 å’Œè¢«æ‹’ç»å¥–åŠ±æ¨è‡³æœ€å° -1/2 æ¥è¿­ä»£æ±‚è§£çº³ä»€å‡è¡¡ï¼Œå¹¶å¯ä»¥ç¼“è§£æ•°æ®ç¨€ç–é—®é¢˜ã€‚è¯¥å®ç°é€šè¿‡ä½¿ç”¨ç¡¬æ ‡ç­¾æ¦‚ç‡æ¥è¿‘ä¼¼è¿™ä¸ªç®—æ³•ï¼Œå°† 1 åˆ†é…ç»™è·èƒœè€…ï¼Œå°† 0 åˆ†é…ç»™å¤±è´¥è€…ã€‚                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| `"aot"` æˆ– `loss_type="aot_pair"`     | [AOT](https://huggingface.co/papers/2406.05882) ä½œè€…æå‡ºä½¿ç”¨é€šè¿‡æœ€ä¼˜ä¼ è¾“çš„åˆ†å¸ƒåå¥½å¯¹é½ã€‚ä¼ ç»Ÿä¸Šï¼Œå¯¹é½ç®—æ³•åœ¨æ ·æœ¬çº§åˆ«ä½¿ç”¨é…å¯¹åå¥½ï¼Œè¿™ä¸ç¡®ä¿åˆ†å¸ƒçº§åˆ«çš„å¯¹é½ã€‚å¦ä¸€æ–¹é¢ï¼ŒAOT å¯ä»¥é€šè¿‡ä½¿æ­£æ ·æœ¬çš„å¥–åŠ±åˆ†å¸ƒåœ¨è´Ÿæ ·æœ¬åˆ†å¸ƒä¸Šåœ¨ä¸€é˜¶ä¸Šéšæœºå ä¼˜æ¥å¯¹é½é…å¯¹æˆ–æœªé…å¯¹åå¥½æ•°æ®çš„ LLMã€‚å…·ä½“æ¥è¯´ï¼Œ`loss_type="aot"` é€‚ç”¨äºé…å¯¹æ•°æ®é›†ï¼Œå…¶ä¸­æ¯ä¸ªæç¤ºéƒ½æœ‰æ‰€é€‰å’Œè¢«æ‹’ç»çš„å“åº”ï¼›`loss_type="aot_pair"` é€‚ç”¨äºæœªé…å¯¹æ•°æ®é›†ã€‚ç®€è€Œè¨€ä¹‹ï¼Œ`loss_type="aot"` ç¡®ä¿å¯¹é½æ¨¡å‹æ‰€é€‰ä¸è¢«æ‹’ç»çš„å¯¹æ•°ä¼¼ç„¶æ¯”å…·æœ‰æ¯”å‚è€ƒæ¨¡å‹è¯¥æ¯”ç‡æ›´é«˜çš„åˆ†ä½æ•°ã€‚`loss_type="aot_pair"` ç¡®ä¿æ‰€é€‰å¥–åŠ±åœ¨æ‰€æœ‰åˆ†ä½æ•°ä¸Šéƒ½é«˜äºè¢«æ‹’ç»å¥–åŠ±ã€‚æ³¨æ„ï¼Œåœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹ï¼Œåˆ†ä½æ•°éƒ½æ˜¯é€šè¿‡æ’åºè·å¾—çš„ã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨ AOT ç®—æ³•çš„ä¼˜åŠ¿ï¼Œæœ€å¤§åŒ–æ¯ GPU æ‰¹æ¬¡å¤§å°å¾ˆé‡è¦ã€‚ |
| `"apo_zero"` æˆ– `loss_type="apo_down"` | [APO](https://huggingface.co/papers/2408.06266) æ–¹æ³•å¼•å…¥äº†å¯¹é½ç›®æ ‡çš„"é”šå®š"ç‰ˆæœ¬ã€‚æœ‰ä¸¤ä¸ªå˜ä½“ï¼š`apo_zero` å’Œ `apo_down`ã€‚`apo_zero` æŸå¤±å¢åŠ è·èƒœè¾“å‡ºçš„ä¼¼ç„¶ï¼ŒåŒæ—¶å‡å°‘å¤±è´¥è¾“å‡ºçš„ä¼¼ç„¶ï¼Œå½“æ¨¡å‹æ€§èƒ½ä¸å¦‚è·èƒœè¾“å‡ºæ—¶é€‚ç”¨ã€‚å¦ä¸€æ–¹é¢ï¼Œ`apo_down` å‡å°‘è·èƒœå’Œå¤±è´¥è¾“å‡ºçš„ä¼¼ç„¶ï¼Œä½†æ›´å¼ºè°ƒå‡å°‘å¤±è´¥è¾“å‡ºçš„ä¼¼ç„¶ã€‚å½“æ¨¡å‹æ¯”è·èƒœè¾“å‡ºæ›´å¥½æ—¶ï¼Œè¿™ä¸ªå˜ä½“æ›´æœ‰æ•ˆã€‚                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| `"discopop"`                           | [DiscoPOP](https://huggingface.co/papers/2406.08414) è®ºæ–‡ä½¿ç”¨ LLM å‘ç°æ›´é«˜æ•ˆçš„ç¦»çº¿åå¥½ä¼˜åŒ–æŸå¤±ã€‚åœ¨è®ºæ–‡ä¸­ï¼Œæå‡ºçš„ DiscoPOP æŸå¤±ï¼ˆè¿™æ˜¯ä¸€ä¸ªå¯¹æ•°æ¯”ç‡è°ƒåˆ¶æŸå¤±ï¼‰åœ¨ä¸åŒä»»åŠ¡ï¼ˆIMDb æ­£é¢æ–‡æœ¬ç”Ÿæˆã€Reddit TLDR æ‘˜è¦å’Œ Alpaca Eval 2.0ï¼‰ä¸Šä¼˜äºå…¶ä»–ä¼˜åŒ–æŸå¤±ã€‚                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |

### æ ‡ç­¾å¹³æ»‘

[cDPO](https://ericmitchell.ai/cdpo.pdf) æ˜¯ DPO æŸå¤±çš„ä¸€ä¸ªè°ƒæ•´ï¼Œæˆ‘ä»¬å‡è®¾åå¥½æ ‡ç­¾ä»¥æŸç§æ¦‚ç‡æœ‰å™ªå£°ã€‚åœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œ[`DPOConfig`] ä¸­çš„ `label_smoothing` å‚æ•°ç”¨äºå»ºæ¨¡ç°æœ‰æ ‡ç­¾å™ªå£°çš„æ¦‚ç‡ã€‚è¦åº”ç”¨è¿™ç§ä¿å®ˆæŸå¤±ï¼Œè¯·å°† `label_smoothing` è®¾ç½®ä¸ºå¤§äº 0.0 çš„å€¼ï¼ˆåœ¨ 0.0 å’Œ 0.5 ä¹‹é—´ï¼›é»˜è®¤å€¼ä¸º 0.0ï¼‰ã€‚

### åŒæ­¥å‚è€ƒæ¨¡å‹

[TR-DPO](https://huggingface.co/papers/2404.09656) è®ºæ–‡å»ºè®®åœ¨ DPO è®­ç»ƒæœŸé—´ï¼Œåœ¨æ¯ `ref_model_sync_steps` æ­¥ SGD åä½¿ç”¨æƒé‡ `ref_model_mixup_alpha` åŒæ­¥å‚è€ƒæ¨¡å‹æƒé‡ã€‚è¦åœ¨ [`DPOConfig`] ä¸­åˆ‡æ¢æ­¤å›è°ƒï¼Œè¯·ä½¿ç”¨ `sync_ref_model=True`ã€‚

### RPO æŸå¤±

[RPO](https://huggingface.co/papers/2404.19733) è®ºæ–‡ä½¿ç”¨ä¸æœ¬æ–‡ [paper](https://huggingface.co/papers/2405.16436) ä¸­ RPO æŸå¤±ç›¸å…³çš„æŸå¤±å®ç°è¿­ä»£åå¥½è°ƒä¼˜ç®—æ³•ï¼Œè¯¥ç®—æ³•æœ¬è´¨ä¸ŠåŒ…æ‹¬å¯¹æ‰€é€‰åå¥½çš„åŠ æƒ SFT æŸå¤±ä»¥åŠ DPO æŸå¤±ã€‚è¦ä½¿ç”¨æ­¤æŸå¤±ï¼Œè¯·åœ¨ [`DPOConfig`] ä¸­å°† `rpo_alpha` è®¾ç½®ä¸ºé€‚å½“çš„å€¼ã€‚è®ºæ–‡å»ºè®®å°†æ­¤æƒé‡è®¾ç½®ä¸º `1.0`ã€‚

### WPO æŸå¤±

[WPO](https://huggingface.co/papers/2406.11827) è®ºæ–‡é€šè¿‡æ ¹æ®å½“å‰ç­–ç•¥ä¸‹çš„æ¦‚ç‡é‡æ–°åŠ æƒåå¥½å¯¹ï¼Œä½¿ç¦»ç­–ç•¥æ•°æ®æ›´æ¥è¿‘åœ¨ç­–ç•¥æ•°æ®ã€‚è¦ä½¿ç”¨æ­¤æ–¹æ³•ï¼Œè¯·åœ¨ [`DPOConfig`] ä¸­å°† `use_weighting` æ ‡å¿—è®¾ç½®ä¸º `True`ã€‚

### LD-DPO æŸå¤±

[LD-DPO](https://huggingface.co/papers/2409.06411) è®ºæ–‡åŸºäºæ··åˆç³»æ•° \\( \alpha \\) å°†è¶…è¿‡æœŸæœ›é•¿åº¦çš„å“åº”éƒ¨åˆ†åˆ†è§£ä¸ºä¸¤ä¸ªç»„ä»¶â€”â€”ç±»äººåå¥½å’Œå†—é•¿åå¥½ã€‚è¦ä½¿ç”¨æ­¤æ–¹æ³•ï¼Œè¯·åœ¨ [`DPOConfig`] ä¸­å°† `ld_alpha` è®¾ç½®ä¸ºé€‚å½“çš„å€¼ã€‚è®ºæ–‡å»ºè®®å°†æ­¤å€¼è®¾ç½®åœ¨ `0.0` å’Œ `1.0` ä¹‹é—´ã€‚

### å¯¹äºä¸“å®¶æ··åˆæ¨¡å‹ï¼šå¯ç”¨è¾…åŠ©æŸå¤±

å¦‚æœè´Ÿè½½åœ¨ä¸“å®¶ä¹‹é—´å¤§è‡´å¹³å‡åˆ†é…ï¼ŒMOE æ˜¯æœ€æœ‰æ•ˆçš„ã€‚  
ä¸ºäº†ç¡®ä¿æˆ‘ä»¬åœ¨åå¥½è°ƒä¼˜æœŸé—´ç±»ä¼¼åœ°è®­ç»ƒ MOEï¼Œå°†è´Ÿè½½å¹³è¡¡å™¨çš„è¾…åŠ©æŸå¤±æ·»åŠ åˆ°æœ€ç»ˆæŸå¤±æ˜¯æœ‰ç›Šçš„ã€‚

æ­¤é€‰é¡¹é€šè¿‡åœ¨æ¨¡å‹é…ç½®ä¸­è®¾ç½® `output_router_logits=True`ï¼ˆä¾‹å¦‚ [`~transformers.MixtralConfig`]ï¼‰æ¥å¯ç”¨ã€‚  
è¦ç¼©æ”¾è¾…åŠ©æŸå¤±å¯¹æ€»æŸå¤±çš„è´¡çŒ®ç¨‹åº¦ï¼Œè¯·åœ¨æ¨¡å‹é…ç½®ä¸­ä½¿ç”¨è¶…å‚æ•° `router_aux_loss_coef=...`ï¼ˆé»˜è®¤å€¼ï¼š`0.001`ï¼‰ã€‚

## ä½¿ç”¨ `unsloth` åŠ é€Ÿ DPO å¾®è°ƒ

æ‚¨å¯ä»¥ä½¿ç”¨ä¸ `SFTTrainer` å®Œå…¨å…¼å®¹çš„ [`unsloth`](https://github.com/unslothai/unsloth) åº“è¿›ä¸€æ­¥åŠ é€Ÿ QLoRA / LoRAï¼ˆ2å€æ›´å¿«ï¼Œå†…å­˜å‡å°‘ 60%ï¼‰ã€‚ç›®å‰ `unsloth` ä»…æ”¯æŒ Llamaï¼ˆYiã€TinyLlamaã€Qwenã€Deepseek ç­‰ï¼‰å’Œ Mistral æ¶æ„ã€‚ä¸‹é¢åˆ—å‡ºäº†ä¸€äº› DPO çš„åŸºå‡†æµ‹è¯•ï¼š

| GPU      | æ¨¡å‹     | æ•°æ®é›†    | ğŸ¤—   | ğŸ¤— + Flash Attention 2 | ğŸ¦¥ Unsloth | ğŸ¦¥ VRAM èŠ‚çœ |
| -------- | --------- | ---------- | --- | --------------------- | --------- | ------------ |
| A100 40G | Zephyr 7b | Ultra Chat | 1x  | 1.24x                 | **1.88x** | -11.6%       |
| Tesla T4 | Zephyr 7b | Ultra Chat | 1x  | 1.09x                 | **1.55x** | -18.6%       |

é¦–å…ˆæ ¹æ® [å®˜æ–¹æ–‡æ¡£](https://github.com/unslothai/unsloth) å®‰è£… `unsloth`ã€‚å®‰è£…åï¼Œæ‚¨å¯ä»¥ä»¥éå¸¸ç®€å•çš„æ–¹å¼å°† unsloth é›†æˆåˆ°æ‚¨çš„å·¥ä½œæµç¨‹ä¸­ï¼›æ‚¨åªéœ€è¦åŠ è½½ `FastLanguageModel` è€Œä¸æ˜¯ `AutoModelForCausalLM`ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```diff
  from datasets import load_dataset
  from trl import DPOConfig, DPOTrainer
- from transformers import AutoModelForCausalLM, AutoTokenizer
+ from unsloth import FastLanguageModel

- model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2-0.5B-Instruct")
- tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2-0.5B-Instruct")
+ model, tokenizer = FastLanguageModel.from_pretrained("Qwen/Qwen2-0.5B-Instruct")
+ model = FastLanguageModel.get_peft_model(model)
  train_dataset = load_dataset("trl-lib/ultrafeedback_binarized", split="train")

- training_args = DPOConfig(output_dir="Qwen2-0.5B-DPO", logging_steps=10)
+ training_args = DPOConfig(output_dir="Qwen2-0.5B-DPO", logging_steps=10, bf16=True)
  trainer = DPOTrainer(model=model, args=training_args, processing_class=tokenizer, train_dataset=train_dataset)
  trainer.train()

```

ä¿å­˜çš„æ¨¡å‹ä¸ Hugging Face çš„ transformers åº“å®Œå…¨å…¼å®¹ã€‚åœ¨ [å®˜æ–¹ä»“åº“](https://github.com/unslothai/unsloth) ä¸­äº†è§£æ›´å¤šå…³äº unsloth çš„ä¿¡æ¯ã€‚

## ä½¿ç”¨ PEFT çš„å‚è€ƒæ¨¡å‹è€ƒè™‘

å¯¹äºå‚è€ƒæ¨¡å‹åœ¨ä½¿ç”¨ PEFT æ—¶å¦‚ä½•å·¥ä½œï¼Œæ‚¨æœ‰ä¸‰ä¸ªä¸»è¦é€‰é¡¹ï¼ˆåŠ ä¸Šå‡ ä¸ªå˜ä½“ï¼‰ï¼Œå‡è®¾æ‚¨æƒ³è¦ç”¨ DPO è¿›ä¸€æ­¥å¢å¼ºçš„æ¨¡å‹æ˜¯ä½¿ç”¨ (Q)LoRA è°ƒä¼˜çš„ã€‚

1. ç®€å•åœ°åˆ›å»ºæ¨¡å‹çš„ä¸¤ä¸ªå®ä¾‹ï¼Œæ¯ä¸ªéƒ½åŠ è½½æ‚¨çš„é€‚é…å™¨ - å·¥ä½œæ­£å¸¸ä½†æ•ˆç‡å¾ˆä½ã€‚
2. å°†é€‚é…å™¨åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹ä¸­ï¼Œåœ¨é¡¶éƒ¨åˆ›å»ºå¦ä¸€ä¸ªé€‚é…å™¨ï¼Œç„¶åå°† `ref_model` å‚æ•°ç•™ç©ºï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ DPOTrainer å°†å¸è½½é€‚é…å™¨è¿›è¡Œå‚è€ƒæ¨ç† - é«˜æ•ˆï¼Œä½†ä¸‹é¢è®¨è®ºäº†æ½œåœ¨çš„ç¼ºç‚¹ã€‚
3. ç”¨ä¸åŒçš„åç§°åŠ è½½é€‚é…å™¨ä¸¤æ¬¡ï¼Œç„¶ååœ¨è®­ç»ƒæœŸé—´ä½¿ç”¨ `set_adapter` åœ¨æ­£åœ¨ DPO çš„é€‚é…å™¨å’Œå‚è€ƒé€‚é…å™¨ä¹‹é—´äº¤æ¢ - ä¸é€‰é¡¹ 2 ç›¸æ¯”æ•ˆç‡ç¨ä½ï¼ˆ~é€‚é…å™¨å¤§å° VRAM å¼€é”€ï¼‰ï¼Œä½†é¿å…äº†é™·é˜±ã€‚

### åœ¨ DPO ä¹‹å‰åˆå¹¶ QLoRA çš„ç¼ºç‚¹ï¼ˆé€‰é¡¹ 2ï¼‰

æ­£å¦‚ [Benjamin Marie](https://medium.com/@bnjmn_marie/dont-merge-your-lora-adapter-into-a-4-bit-llm-65b6da287997) æ‰€å»ºè®®çš„ï¼Œåˆå¹¶ QLoRA é€‚é…å™¨çš„æœ€ä½³é€‰æ‹©æ˜¯é¦–å…ˆåé‡åŒ–åŸºç¡€æ¨¡å‹ï¼Œç„¶ååˆå¹¶é€‚é…å™¨ã€‚ç±»ä¼¼äº [è¿™ä¸ªè„šæœ¬](https://github.com/jondurbin/qlora/blob/main/qmerge.py)ã€‚

ä½†æ˜¯ï¼Œä½¿ç”¨è¿™ç§æ–¹æ³•åï¼Œæ‚¨å°†æœ‰ä¸€ä¸ªæœªé‡åŒ–çš„åŸºç¡€æ¨¡å‹ã€‚å› æ­¤ï¼Œè¦å¯¹ DPO ä½¿ç”¨ QLoRAï¼Œæ‚¨éœ€è¦é‡æ–°é‡åŒ–åˆå¹¶çš„æ¨¡å‹æˆ–ä½¿ç”¨æœªé‡åŒ–çš„åˆå¹¶ï¼ˆå¯¼è‡´æ›´é«˜çš„å†…å­˜éœ€æ±‚ï¼‰ã€‚

### ä½¿ç”¨é€‰é¡¹ 3 - åŠ è½½é€‚é…å™¨ä¸¤æ¬¡

ä¸ºäº†é¿å…é€‰é¡¹ 2 çš„ç¼ºç‚¹ï¼Œæ‚¨å¯ä»¥å°†å¾®è°ƒçš„é€‚é…å™¨åŠ è½½åˆ°æ¨¡å‹ä¸­ä¸¤æ¬¡ï¼Œä½¿ç”¨ä¸åŒçš„åç§°ï¼Œå¹¶åœ¨ [`DPOTrainer`] ä¸­è®¾ç½®æ¨¡å‹/å‚è€ƒé€‚é…å™¨åç§°ã€‚

ä¾‹å¦‚ï¼š

```python
# åŠ è½½åŸºç¡€æ¨¡å‹ã€‚
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
)
model = AutoModelForCausalLM.from_pretrained(
    "mistralai/mixtral-8x7b-v0.1",
    load_in_4bit=True,
    quantization_config=bnb_config,
    attn_implementation="flash_attention_2",
    torch_dtype=torch.bfloat16,
    device_map="auto",
)
model.config.use_cache = False

# åŠ è½½é€‚é…å™¨ã€‚
model = PeftModel.from_pretrained(
    model,
    "/path/to/peft",
    is_trainable=True,
    adapter_name="train",
)
# ç”¨ä¸åŒçš„åç§°ç¬¬äºŒæ¬¡åŠ è½½é€‚é…å™¨ï¼Œè¿™å°†æ˜¯æˆ‘ä»¬çš„å‚è€ƒæ¨¡å‹ã€‚
model.load_adapter("/path/to/peft", adapter_name="reference")

# åˆå§‹åŒ–è®­ç»ƒå™¨ï¼Œæ²¡æœ‰ ref_model å‚æ•°ã€‚
training_args = DPOConfig(
    model_adapter_name="train",
    ref_adapter_name="reference",
)
dpo_trainer = DPOTrainer(
    model,
    args=training_args,
    ...
)
```

## DPOTrainer

[[autodoc]] DPOTrainer

## DPOConfig

[[autodoc]] DPOConfig

## DataCollatorForPreference

[[autodoc]] trainer.dpo_trainer.DataCollatorForPreference 