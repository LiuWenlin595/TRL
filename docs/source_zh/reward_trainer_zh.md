# å¥–åŠ±å»ºæ¨¡

[![](https://img.shields.io/badge/All_models-Reward_Trainer-blue)](https://huggingface.co/models?other=reward-trainer,trl)

TRL æ”¯æŒè‡ªå®šä¹‰å¥–åŠ±å»ºæ¨¡ï¼Œä»»ä½•äººéƒ½å¯ä»¥åœ¨è‡ªå·±çš„æ•°æ®é›†å’Œæ¨¡å‹ä¸Šæ‰§è¡Œå¥–åŠ±å»ºæ¨¡ã€‚

æŸ¥çœ‹å®Œæ•´çš„çµæ´»ç¤ºä¾‹ [`examples/scripts/reward_modeling.py`](https://github.com/huggingface/trl/tree/main/examples/scripts/reward_modeling.py)ã€‚

## é¢„æœŸæ•°æ®é›†ç±»å‹

[`RewardTrainer`] éœ€è¦ [*éšå¼æç¤º*åå¥½æ•°æ®é›†](dataset_formats#preference)ã€‚è¿™æ„å‘³ç€æ•°æ®é›†åº”è¯¥åªåŒ…å« `"chosen"` å’Œ `"rejected"` åˆ—ï¼ˆè€Œä¸æ˜¯ `"prompt"`ï¼‰ã€‚
[`RewardTrainer`] æ”¯æŒ [å¯¹è¯å¼](dataset_formats#conversational) å’Œ [æ ‡å‡†](dataset_formats#standard) æ•°æ®é›†æ ¼å¼ã€‚å½“æä¾›å¯¹è¯å¼æ•°æ®é›†æ—¶ï¼Œè®­ç»ƒå™¨å°†è‡ªåŠ¨å¯¹æ•°æ®é›†åº”ç”¨èŠå¤©æ¨¡æ¿ã€‚

æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨é¢„æ ‡è®°åŒ–çš„æ•°æ®é›†ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ•°æ®é›†åº”åŒ…å«ä»¥ä¸‹åˆ—ï¼š`input_ids_chosen`ã€`attention_mask_chosen`ã€`input_ids_rejected` å’Œ `attention_mask_rejected`ã€‚

## ä½¿ç”¨ `RewardTrainer`

å‡†å¤‡æ•°æ®é›†åï¼Œæ‚¨å¯ä»¥åƒä½¿ç”¨ ğŸ¤— Transformers ä¸­çš„ `Trainer` ç±»ä¸€æ ·ä½¿ç”¨ [`RewardTrainer`]ã€‚
æ‚¨åº”è¯¥å°† `AutoModelForSequenceClassification` æ¨¡å‹ä¼ é€’ç»™ [`RewardTrainer`]ï¼ŒåŒæ—¶ä¼ é€’é…ç½®è®­ç»ƒè¶…å‚æ•°çš„ [`RewardConfig`]ã€‚

### åˆ©ç”¨ ğŸ¤— PEFT è®­ç»ƒå¥–åŠ±æ¨¡å‹

åªéœ€åœ¨ [`RewardTrainer`] çš„å…³é”®å­—å‚æ•°ä¸­ä¼ é€’ `peft_config`ï¼Œè®­ç»ƒå™¨åº”è¯¥è‡ªåŠ¨å¤„ç†å°†æ¨¡å‹è½¬æ¢ä¸º PEFT æ¨¡å‹ï¼

```python
from peft import LoraConfig, TaskType
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from trl import RewardTrainer, RewardConfig

model = AutoModelForSequenceClassification.from_pretrained("gpt2")
peft_config = LoraConfig(
    task_type=TaskType.SEQ_CLS,
    inference_mode=False,
    r=8,
    lora_alpha=32,
    lora_dropout=0.1,
)

...

trainer = RewardTrainer(
    model=model,
    args=training_args,
    processing_class=tokenizer,
    train_dataset=dataset,
    peft_config=peft_config,
)

trainer.train()

```

### ä¸ºæŸå¤±å‡½æ•°æ·»åŠ è¾¹ç•Œ

å¦‚ [Llama 2 è®ºæ–‡](https://huggingface.co/papers/2307.09288) ä¸­æ‰€è¿°ï¼Œæ‚¨å¯ä»¥é€šè¿‡å‘æ•°æ®é›†æ·»åŠ  `margin` åˆ—ä¸ºæŸå¤±å‡½æ•°æ·»åŠ è¾¹ç•Œã€‚å¥–åŠ±æ•´ç†å™¨å°†è‡ªåŠ¨ä¼ é€’å®ƒï¼Œå¹¶ç›¸åº”åœ°è®¡ç®—æŸå¤±ã€‚

```python
def add_margin(row):
    # å‡è®¾æ‚¨æœ‰ score_chosen å’Œ score_rejected åˆ—ï¼Œæ‚¨æƒ³ç”¨å®ƒä»¬æ¥è®¡ç®—è¾¹ç•Œ
    return {'margin': row['score_chosen'] - row['score_rejected']}

dataset = dataset.map(add_margin)
```

### å±…ä¸­å¥–åŠ±

åœ¨è®¸å¤šåœºæ™¯ä¸­ï¼Œæœ€å¥½ç¡®ä¿å¥–åŠ±æ¨¡å‹çš„è¾“å‡ºå‡å€¼ä¸ºé›¶ã€‚è¿™é€šå¸¸é€šè¿‡é¦–å…ˆè®¡ç®—æ¨¡å‹çš„å¹³å‡åˆ†æ•°ç„¶åå‡å»å®ƒæ¥å®Œæˆã€‚

[[Eisenstein et al., 2023]](https://huggingface.co/papers/2312.09244) æå‡ºäº†ä¸€ä¸ªè¾…åŠ©æŸå¤±å‡½æ•°ï¼Œæ—¨åœ¨ç›´æ¥å­¦ä¹ å±…ä¸­çš„å¥–åŠ±æ¨¡å‹ã€‚è¿™ä¸ªè¾…åŠ©æŸå¤±æœ€å°åŒ–å¥–åŠ±çš„å¹³æ–¹å’Œï¼Œé¼“åŠ±æ¨¡å‹è‡ªç„¶åœ°äº§ç”Ÿå‡å€¼ä¸ºé›¶çš„è¾“å‡ºï¼š

$$\Big( R(p, r_1) + R(p, r_2) \Big)^2 $$

è¿™ä¸ªè¾…åŠ©æŸå¤±ä¸ä¸»æŸå¤±å‡½æ•°ç»“åˆï¼Œç”± `[RewardConfig]` ä¸­çš„å‚æ•° `center_rewards_coefficient` åŠ æƒã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œæ­¤åŠŸèƒ½è¢«åœç”¨ï¼ˆ`center_rewards_coefficient = None`ï¼‰ã€‚

```python
training_args = RewardConfig(
    center_rewards_coefficient=0.01,
    ...
)
```

æœ‰å…³å‚è€ƒç»“æœï¼Œè¯·å‚é˜… PR [#1932](https://github.com/huggingface/trl/pull/1932)ã€‚

## RewardTrainer

[[autodoc]] RewardTrainer

## RewardConfig

[[autodoc]] RewardConfig 