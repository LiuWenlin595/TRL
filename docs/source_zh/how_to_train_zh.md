# 训练常见问题

## 我应该关注哪些指标？

在进行传统的语言模型监督微调时，损失（特别是验证损失）是训练进展的良好指标。然而，在强化学习（RL）中，损失对模型性能的指示性变得较差，其值可能会波动，而实际性能却在改善。

为了解决这个问题，我们建议首先关注两个关键指标：

**平均奖励**：主要目标是最大化模型在RL训练期间获得的奖励。
**目标KL散度**：KL散度（Kullback-Leibler散度）衡量两个概率分布之间的差异。在RL训练中，我们用它来量化当前模型与参考模型之间的差异。理想情况下，我们希望将KL散度保持在0到10之间，以确保模型生成的文本与参考模型产生的文本保持接近。

然而，还有更多指标对调试有用，请查看[日志部分](logging)。

## 为什么我们使用参考模型，KL散度的目的是什么？

在训练RL模型时，仅针对奖励进行优化可能导致意外行为，模型可能以不符合良好语言生成的方式利用环境。在RLHF的情况下，我们使用经过训练的奖励模型来预测生成的文本是否被人类高度评价。

【important】
然而，针对奖励模型进行优化的RL模型可能学习到产生高奖励但不代表良好语言的模式。这可能导致极端情况，模型生成带有过多感叹号或表情符号的文本来最大化奖励。在一些最坏的情况下，模型可能生成与自然语言完全无关的模式，但仍获得高奖励，类似于对抗攻击。

<div style="text-align: center">
<img src="https://huggingface.co/datasets/trl-lib/documentation-images/resolve/main/kl-example.png">
<p style="text-align: center;"> <b>图：</b> 来自 <a href="https://huggingface.co/papers/1909.08593">https://huggingface.co/papers/1909.08593</a> 的无KL惩罚样本。 </p>
</div>

为了解决这个问题，我们在奖励函数中添加了基于当前模型与参考模型之间KL散度的惩罚。通过这样做，我们鼓励模型保持接近参考模型生成的内容。

## 负KL散度有什么问题？

如果你纯粹从模型分布中采样生成文本，通常一切正常。但是当你使用`generate`方法时，有一些注意事项，因为它并不总是纯粹采样，这取决于设置，可能导致KL散度变为负值。本质上，当活跃模型达到`log_p_token_active < log_p_token_ref`时，我们得到负KL散度。这可能在几种情况下发生：

【important】
- **top-k采样**：模型可以平滑概率分布，导致top-k token的概率比参考模型的小，但它们仍然被选中
- **min_length**：这会在达到`min_length`之前忽略EOS token。因此模型可以为EOS token分配非常低的log概率，为所有其他token分配非常高的概率，直到达到min_length

这些只是几个例子。为什么负KL是个问题？总奖励`R`计算为`R = r - beta * KL`，所以如果模型学会如何使KL散度变为负值，它实际上获得了正奖励。在许多情况下，利用生成中的这种bug可能比实际学习奖励函数更容易。此外，KL可以变得任意小，因此实际奖励与它相比可能非常小。

那么你应该如何为PPO训练生成文本呢？让我们来看看！

## 如何为训练生成文本？

为了避免上述KL问题，我们建议使用以下设置：

```python
generation_kwargs = {
    "min_length": -1, # 不忽略EOS token（见上文）
    "top_k": 0.0, # 无top-k采样
    "top_p": 1.0, # 无核采样
    "do_sample": True, # 是的，我们想要采样
    "pad_token_id": tokenizer.eos_token_id, # 大多数解码器模型没有padding token - 使用EOS token代替
    "max_new_tokens": 32, # 指定你最多想要生成多少个token
}
```

使用这些设置，我们通常不会遇到任何问题。你也可以尝试其他设置，但如果遇到负KL散度问题，尝试回到这些设置，看看问题是否仍然存在。

## 如何调试你自己的用例？

由于RL管道的复杂性，调试可能具有挑战性。以下是一些使过程更容易的技巧和建议：

- **从工作示例开始**：从trl仓库的工作示例开始，逐渐修改以适应你的特定用例。一次性改变所有内容可能使识别潜在问题的来源变得困难。例如，你可以从替换示例中的模型开始，一旦你找到最佳超参数，尝试切换到你的数据集和奖励模型。如果你一次性改变所有内容，你就不会知道潜在问题来自哪里。
- **从小开始，稍后扩展**：训练大型模型可能非常慢，需要几个小时或几天才能看到任何改善。对于调试来说，这不是一个方便的时间尺度，所以在开发阶段尝试使用小型模型变体，一旦工作正常再扩展。话虽如此，有时你必须小心，因为小型模型可能没有解决复杂任务的能力。
- **从简单开始**：尝试从最小示例开始，然后从那里构建复杂性。你的用例可能需要例如由许多不同奖励组成的复杂奖励函数 - 尝试首先使用一个信号，看看你是否可以优化它，然后在那之后添加更多复杂性。
- **检查生成内容**：检查模型正在生成什么总是一个好主意。也许你的后处理或提示中有bug。由于设置不当，你可能过早地截断生成内容。这些事情在指标上很难看到，但如果你看生成内容就很明显。
- **检查奖励模型**：如果你的奖励没有随时间改善，也许奖励模型有问题。你可以查看极端情况，看看它是否做了应该做的事情：例如，在情感情况下，你可以检查简单的正面和负面示例是否真的得到不同的奖励。你可以查看数据集的分布。最后，也许奖励被模型无法影响的查询主导，所以你可能需要标准化这个（例如，查询+响应的奖励减去查询的奖励）。

这些只是我们发现有用的一些技巧 - 如果你有更多有用的技巧，欢迎打开PR来添加它们！ 